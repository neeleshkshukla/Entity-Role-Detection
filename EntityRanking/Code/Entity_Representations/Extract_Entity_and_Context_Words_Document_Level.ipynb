{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implement is to extract entity and surrounding word and give it representative tag.\n",
    "Say Entity E is made of words [e1, e2... ek]. Lets take -d and +d words as context. \n",
    "\n",
    "making e1-d......[e1, e2. .....ek]... ek+d as total entity words.\n",
    "\n",
    "We would store doc_name, tag, entity and entity_context\n",
    "\n",
    "Dictionary format\n",
    "\n",
    "Element of Dictionary [ doc_id: dictonary of roles]\n",
    "\n",
    "Key: Document names present in the corpus\n",
    "Values: Dictionary of roles present in specific directory\n",
    "\n",
    "Elements of Role Dictionary: \n",
    "Keys: Roles present in a specific doc\n",
    "Value: List of entities having that role\n",
    "\n",
    "Entity list itself is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir_train = '../../Data/input/train'\n",
    "corpus_dir_test = '../../Data/input/test'\n",
    "content_dir_name = 'content'\n",
    "tag_dir_name = 'new_tags'\n",
    "\n",
    "train_content_dir = os.path.join(corpus_dir_train, content_dir_name)\n",
    "train_tag_dir = os.path.join(corpus_dir_train, tag_dir_name)\n",
    "test_content_dir = os.path.join(corpus_dir_test, content_dir_name)\n",
    "test_tag_dir = os.path.join(corpus_dir_test, tag_dir_name)\n",
    "\n",
    "win_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_ROLES = dict()\n",
    "\n",
    "ENTITY_ROLES['PER_Victim'] = 0\n",
    "ENTITY_ROLES['PER_Others'] = 0\n",
    "ENTITY_ROLES['LOC_Event'] = 0\n",
    "ENTITY_ROLES['ORG_Accused'] = 0\n",
    "ENTITY_ROLES['ORG_Victim'] = 0\n",
    "ENTITY_ROLES['LOC_Others'] = 0\n",
    "ENTITY_ROLES['ORG_Others'] = 0\n",
    "ENTITY_ROLES['PER_Accused'] = 0\n",
    "ENTITY_ROLES['LOC_Accused'] = 0\n",
    "ENTITY_ROLES['LOC_Victim'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_role_entity_word_dictionary(content_dir, tag_dir, win_len):\n",
    "    \n",
    "    doc_role_entity_word_dict = dict()\n",
    "    \n",
    "    \n",
    "    for f in os.listdir(content_dir):\n",
    "        #print(f)\n",
    "        \n",
    "        role_entity_dict = dict()\n",
    "        per_victim_list = list()\n",
    "        per_accused_list = list()\n",
    "        per_others_list = list()\n",
    "        org_victim_list = list()\n",
    "        org_accused_list = list()\n",
    "        org_others_list = list()\n",
    "        loc_victim_list = list()\n",
    "        loc_accused_list = list()\n",
    "        loc_others_list = list()\n",
    "        loc_event_list = list()\n",
    "        entity_doc_level_word_dict = dict()\n",
    "    \n",
    "        content_file = os.path.join(content_dir, f)\n",
    "        tag_file = os.path.join(tag_dir, f)\n",
    "        words = list()\n",
    "        tags = list()\n",
    "        with open(content_file, 'rt') as cf:\n",
    "            for line in cf:\n",
    "                for word in line.strip('\\n').split():\n",
    "                    words.append(word)\n",
    "\n",
    "        with open(tag_file, 'rt') as tf:\n",
    "            for line in tf:\n",
    "                for tag in line.strip('\\n').split():\n",
    "                    tags.append(tag)\n",
    "        word_len = len(words)\n",
    "        tag_len = len(tags)\n",
    "        if word_len == tag_len:\n",
    "            i = 0\n",
    "            while i < word_len:\n",
    "        \n",
    "                if tags[i] in ENTITY_ROLES.keys():\n",
    "                    entity_start_index = i\n",
    "                    entity_end_index = i \n",
    "                    while entity_end_index< word_len -1 and tags[entity_end_index] == tags[entity_end_index+1]:\n",
    "                        entity_end_index = entity_end_index + 1\n",
    "                    i = entity_end_index;\n",
    "                    context_start_index = entity_start_index - win_len\n",
    "                    context_end_index = entity_end_index + win_len\n",
    "                    while context_start_index < 0:\n",
    "                        context_start_index = context_start_index + 1\n",
    "                    while context_end_index > word_len:\n",
    "                        context_end_index = context_end_index - 1\n",
    "                    entity_slice = slice(entity_start_index,entity_end_index+1)\n",
    "                    context_slice = slice(context_start_index,context_end_index+1)\n",
    "                    entity_words = '_'.join(words[entity_slice])\n",
    "                    entity_tag = tags[entity_end_index]\n",
    "                    sent_context_words = words[context_slice]\n",
    "                    \n",
    "                    if entity_words in entity_doc_level_word_dict.keys():\n",
    "                        doc_context_words = entity_doc_level_word_dict[entity_words]\n",
    "                        doc_context_words.extend(sent_context_words)\n",
    "                        entity_doc_level_word_dict[entity_words] = doc_context_words\n",
    "                       # print('Occured Again', '\\n')\n",
    "                    else:\n",
    "                        entity_doc_level_word_dict[entity_words] = sent_context_words\n",
    "                   # print(entity_words, entity_doc_level_word_dict[entity_words], '\\n')\n",
    "                    \n",
    "                \n",
    "                    if entity_tag == 'PER_Victim':\n",
    "                        per_victim_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "                \n",
    "                    if entity_tag == 'PER_Others':\n",
    "                        per_others_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'PER_Accused':\n",
    "                        per_accused_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'LOC_Others':\n",
    "                        loc_others_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'LOC_Event':\n",
    "                        loc_event_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'LOC_Accused':\n",
    "                        loc_accused_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'LOC_Victim':\n",
    "                        loc_victim_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'ORG_Accused':\n",
    "                        org_accused_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'ORG_Victim':\n",
    "                        org_victim_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "\n",
    "                    if entity_tag == 'ORG_Others':\n",
    "                        org_others_list.append((entity_words, entity_doc_level_word_dict[entity_words]))\n",
    "                \n",
    "                    #print(words[entity_slice], tags[entity_end_index])\n",
    "                    #print(words[context_slice])\n",
    "                    #print('\\n')\n",
    "                i = i+1\n",
    "            \n",
    "        role_entity_dict['PER_Victim'] = per_victim_list\n",
    "        role_entity_dict['PER_Others'] = per_others_list\n",
    "        role_entity_dict['PER_Accused'] = per_accused_list\n",
    "        role_entity_dict['LOC_Others'] = loc_others_list\n",
    "        role_entity_dict['LOC_Event'] = loc_event_list\n",
    "        role_entity_dict['LOC_Accused'] = loc_accused_list\n",
    "        role_entity_dict['LOC_Victim'] = loc_victim_list\n",
    "        role_entity_dict['ORG_Accused'] = org_accused_list\n",
    "        role_entity_dict['ORG_Victim'] = org_victim_list\n",
    "        role_entity_dict['ORG_Others'] = org_others_list\n",
    "        \n",
    "        doc_role_entity_word_dict[f] = role_entity_dict\n",
    "    \n",
    "    return doc_role_entity_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_entity_word_dict = build_doc_role_entity_word_dictionary(train_content_dir, train_tag_dir, win_len)\n",
    "test_doc_entity_word_dict = build_doc_role_entity_word_dictionary(test_content_dir, test_tag_dir, win_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_doc_entity_word_dict, open('../../Data/output/EntityRep/train/doc_entity_doc_level_context_word.p', 'wb'))\n",
    "pickle.dump(test_doc_entity_word_dict, open('../../Data/output/EntityRep/test/doc_entity_doc_level_context_word.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
