{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n",
      "[  3.00710000e-01  -4.68670000e-01  -2.06170000e-01  -8.09780000e-01\n",
      "  -2.38890000e-01   2.43290000e-01   1.65380000e-02  -3.56870000e-02\n",
      "  -2.23060000e-01   9.51890000e-01  -3.22730000e-01   2.19800000e-01\n",
      "  -6.75240000e-02  -3.72200000e-01  -3.97180000e-01  -4.38610000e-01\n",
      "   1.19670000e-01  -2.99640000e-01   2.84370000e-02  -8.75440000e-02\n",
      "   1.65690000e-01  -4.94510000e-01  -6.20110000e-01  -1.65740000e-01\n",
      "  -9.72180000e-02  -9.94740000e-02  -8.03070000e-02  -3.93380000e-01\n",
      "  -2.41950000e-01   3.20230000e-01  -5.33200000e-01  -4.01840000e-01\n",
      "  -6.71350000e-01  -7.85610000e-02   5.55460000e-01   2.99970000e-01\n",
      "  -9.96500000e-02  -6.70350000e-01   1.26690000e-01  -1.86180000e-01\n",
      "  -6.26210000e-02   4.52900000e-01   3.92650000e-01   2.41210000e-01\n",
      "  -4.14740000e-01  -6.18900000e-01  -1.04120000e-01  -3.10430000e-01\n",
      "  -6.67880000e-03  -8.32480000e-01   6.51500000e-01   9.01810000e-01\n",
      "   2.41460000e-02  -7.07660000e-02  -3.95800000e-01  -3.64870000e-01\n",
      "  -2.39290000e-01  -1.51450000e-01   2.07770000e-01   5.46710000e-01\n",
      "  -2.50420000e-01  -6.01420000e-01  -5.48200000e-01   7.72490000e-03\n",
      "  -5.32880000e-01   5.03250000e-01  -1.27120000e-01   1.19890000e-01\n",
      "  -6.45840000e-01   3.55760000e-01   1.74960000e-01   1.18380000e-01\n",
      "  -3.21810000e-01   7.48140000e-02  -9.03810000e-02  -2.98430000e-01\n",
      "   1.67980000e-02  -1.27350000e-01   7.35670000e-01  -1.73350000e-01\n",
      "   3.71230000e-01   3.79790000e-01  -5.18010000e-01   2.76210000e-01\n",
      "   2.15120000e-01  -8.25880000e-02   2.16380000e-01   1.25950000e-01\n",
      "   3.84360000e-01  -1.33320000e-01   5.71850000e-02  -2.81270000e-01\n",
      "  -4.43100000e-01   1.34980000e-01   1.33060000e-01  -3.20500000e-02\n",
      "   1.97190000e-01   2.54550000e-01   6.34750000e-01  -2.34740000e-01\n",
      "  -3.60380000e-01   4.11480000e-02  -2.44220000e-01   8.37310000e-01\n",
      "  -2.25040000e-01  -2.96830000e-01   6.38980000e-01  -3.97740000e-01\n",
      "  -1.03220000e-01  -1.74460000e-01  -7.80590000e-02   2.64790000e-01\n",
      "  -4.22500000e-01  -1.06710000e-01  -9.64680000e-02  -1.70270000e-01\n",
      "   2.74970000e-01  -1.28130000e-01   2.47510000e-01   2.59990000e-01\n",
      "   1.83270000e-01   1.09880000e-01   3.54860000e-04  -4.90290000e-01\n",
      "   1.95820000e-01  -4.52260000e-01  -1.36170000e-02   1.07650000e-01\n",
      "  -1.61610000e-02  -2.72420000e-01   7.78770000e-02  -1.18600000e-01\n",
      "   9.27920000e-02  -4.37740000e-01  -2.65390000e-01  -2.65900000e-01\n",
      "   8.05850000e-02   1.86260000e-01   1.73620000e-01  -2.02420000e-01\n",
      "   3.53270000e-01  -6.43350000e-02   1.37640000e-01  -4.44170000e-01\n",
      "   8.75210000e-01  -2.32600000e-01  -6.76570000e-01   2.38910000e-01\n",
      "  -8.01760000e-02   4.95260000e-01  -2.85790000e-01   2.50410000e-01\n",
      "   1.58530000e-01  -1.29600000e-01  -5.15290000e-01  -3.31750000e-01\n",
      "   4.18260000e-01   3.32110000e-01  -1.17930000e+00   2.28180000e-01\n",
      "  -5.77550000e-01   7.73140000e-01   1.60930000e-01   2.33600000e-01\n",
      "  -1.87640000e-01  -2.45160000e-01  -5.48030000e-01   2.31100000e-01\n",
      "  -3.29750000e-01  -1.26460000e-01   3.79840000e-01   3.60060000e-01\n",
      "   6.03820000e-01  -1.58820000e-01  -4.36820000e-01  -6.34440000e-01\n",
      "  -2.88300000e-01  -1.36090000e-01  -2.58210000e-02  -4.07670000e-01\n",
      "   1.86360000e-01  -4.58570000e-01  -2.46110000e-01  -4.58900000e-02\n",
      "   8.76130000e-02  -1.56850000e-01   3.01290000e-01  -8.01760000e-01\n",
      "   1.23630000e-01   7.14580000e-03   1.47510000e-01   3.54710000e-01\n",
      "   1.51200000e-01   8.19380000e-02  -3.67110000e-01  -2.72080000e-01\n",
      "  -3.55970000e-01   1.72070000e-01  -4.18500000e-02   4.95470000e-01\n",
      "  -3.16300000e-01   3.49150000e-01  -3.72950000e-02   2.09960000e-01\n",
      "  -3.01030000e-01  -1.08750000e-01   3.03540000e-01   2.81570000e-01\n",
      "  -7.98800000e-02  -5.06110000e-01  -2.94160000e-01  -3.08610000e-01\n",
      "  -8.24620000e-01  -1.00190000e-01   9.04730000e-02  -1.82380000e-01\n",
      "  -5.92230000e-03   6.38330000e-02   1.52100000e-01  -2.53850000e-01\n",
      "  -6.88310000e-01  -3.45490000e-02   4.51800000e-01   6.22930000e-02\n",
      "  -4.63430000e-01   4.04000000e-01   4.51060000e-02   1.73750000e-01\n",
      "  -2.77450000e-02   3.63610000e-01   7.82350000e-02   1.95380000e-01\n",
      "  -1.65060000e-01   3.96270000e-02  -3.01130000e-01   2.22570000e-01\n",
      "   2.67730000e-02   1.91510000e-01   4.99870000e-01  -3.54910000e-01\n",
      "  -1.99280000e-02   9.07010000e-01  -8.54900000e-01  -3.93610000e-01\n",
      "   4.10300000e-01   1.46310000e-01  -1.56640000e-01   5.39710000e-01\n",
      "   1.48690000e-01  -5.51930000e-02  -4.77180000e-01  -3.74980000e-01\n",
      "  -5.32330000e-01   1.23200000e-01  -1.22270000e-01  -5.81570000e-02\n",
      "   2.42450000e-02   1.25770000e-01  -1.85580000e-01   7.50540000e-02\n",
      "  -7.28220000e-01  -3.58780000e-02   1.59890000e-01  -2.57430000e-01\n",
      "   2.18560000e-01  -2.28100000e-01   4.83560000e-02   5.60230000e-02\n",
      "   1.51110000e-01   2.29450000e-01   4.58460000e-01  -8.70560000e-02\n",
      "  -2.96620000e-01   1.50540000e-02   2.11020000e-01  -3.74460000e-02\n",
      "   7.89020000e-01  -3.41930000e-01  -8.24300000e-01  -7.17480000e-01\n",
      "  -1.96490000e-01   8.75700000e-02  -2.05520000e-01   1.46100000e-02\n",
      "  -3.80880000e-01   6.53090000e-01  -2.05610000e-01  -2.84270000e-02\n",
      "   1.05770000e-02   8.84100000e-03   1.19920000e-01   1.46110000e-01\n",
      "   1.60340000e-01   7.24310000e-02  -4.37600000e-01  -2.59790000e-01\n",
      "   5.81580000e-01   4.92670000e-01  -1.12760000e-01  -2.77750000e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load Glove Embedding\n",
    "gloveFile = \"glove.6B/glove.6B.300d.txt\"\n",
    "glove_model = {}\n",
    "\n",
    "with open(gloveFile, 'r') as f:\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        glove_model[word] = embedding\n",
    "    print \"Done.\",len(glove_model),\" words loaded!\"  \n",
    "\n",
    "print glove_model['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.57080000e-02  -4.08090000e-03  -1.71850000e-01   1.57380000e-01\n",
      "  -1.54510000e-01   1.29430000e-01  -3.71710000e-01   2.87720000e-01\n",
      "  -1.77560000e-01  -1.74180000e+00   6.77970000e-01   1.95320000e-01\n",
      "  -2.10470000e-01   3.12780000e-02  -2.72330000e-01  -9.39930000e-02\n",
      "   2.69210000e-01   3.84920000e-01  -5.40090000e-01  -1.63370000e-01\n",
      "   3.35600000e-01   4.30180000e-01   5.00560000e-02   1.74400000e-02\n",
      "  -5.07370000e-02   1.41920000e-01   1.30980000e-01   1.13790000e-01\n",
      "  -3.73490000e-01   2.58930000e-01   3.53490000e-01   1.49580000e-01\n",
      "  -5.85640000e-03  -2.07550000e-01  -1.93410000e-01   4.63010000e-02\n",
      "   3.92770000e-01  -2.71260000e-02  -3.01780000e-01  -5.89790000e-01\n",
      "  -2.54810000e-02   1.23350000e-01   4.40330000e-01   3.42790000e-01\n",
      "  -1.65740000e-01  -2.59720000e-01  -3.33980000e-01   4.92420000e-02\n",
      "  -6.27630000e-02  -3.90970000e-01  -7.25190000e-02   2.46290000e-01\n",
      "   5.15210000e-02   4.74460000e-02  -3.80240000e-01  -1.71270000e-01\n",
      "  -3.02750000e-01   7.29570000e-01  -4.55880000e-02  -2.32660000e-01\n",
      "   4.45110000e-02   3.68980000e-01   6.87840000e-02   3.07620000e-02\n",
      "  -5.92710000e-01   9.16230000e-02   1.97050000e-02   2.32800000e-01\n",
      "  -2.89010000e-01   2.95540000e-01  -2.17190000e-01   3.45850000e-01\n",
      "   2.34270000e-01   2.80520000e-01  -2.03050000e-01  -2.52690000e-01\n",
      "   3.28420000e-01  -3.79880000e-01  -4.57610000e-02   5.41780000e-01\n",
      "  -3.25700000e-01   1.88750000e-01  -4.57980000e-01   1.35760000e-01\n",
      "  -2.60850000e-01   1.53620000e-01  -1.37920000e-01   1.04600000e-01\n",
      "  -3.41910000e-01   9.95600000e-02  -5.09810000e-01   1.28410000e-01\n",
      "  -2.09620000e-01   2.67890000e-01  -4.67400000e-02  -5.08580000e-02\n",
      "  -1.12890000e-01   4.60590000e-02  -1.51910000e-01   2.41750000e-01\n",
      "   2.45110000e-01  -3.98710000e-01   4.19060000e-01  -4.67660000e-01\n",
      "   2.58460000e-01  -1.71010000e-01   3.10260000e-01  -1.95030000e-01\n",
      "   3.43410000e-01   4.87730000e-01  -1.78470000e-01  -3.00120000e-01\n",
      "  -1.88740000e-01   4.61740000e-01   1.24120000e-01  -4.13390000e-01\n",
      "  -1.37010000e-01  -1.35040000e-01   5.95200000e-02   4.34570000e-01\n",
      "  -5.69670000e-02   4.09090000e-01   6.75980000e-01   2.47790000e-01\n",
      "   2.95890000e-01  -1.52300000e-02   3.12670000e-01   1.92370000e-01\n",
      "   8.01340000e-02  -1.81910000e-01  -7.09580000e-02  -5.72070000e-01\n",
      "  -2.34760000e-02  -7.93620000e-01  -2.67930000e-02  -7.60910000e-02\n",
      "  -2.77050000e-01   3.10950000e-01  -5.65220000e-01  -2.89820000e-01\n",
      "  -1.45660000e-01   1.45520000e-01  -1.26980000e-01  -4.62210000e-01\n",
      "   2.28810000e-01   5.26260000e-01   5.65560000e-02   3.74050000e-02\n",
      "  -2.80090000e-01  -3.02720000e-03   6.69990000e-01  -4.38630000e-01\n",
      "  -5.70310000e-03  -3.58170000e-02  -3.78680000e-01   4.72010000e-01\n",
      "   3.54650000e-01  -1.26870000e-01  -6.94010000e-02  -2.17930000e-01\n",
      "  -2.75310000e-01   7.60700000e-01  -2.33760000e-01  -1.47260000e-01\n",
      "   4.41780000e-02  -8.71570000e-01  -3.06890000e-01   2.37990000e-01\n",
      "  -3.12740000e-01   5.95630000e-01  -2.59220000e-01  -4.49890000e-01\n",
      "  -1.02610000e+00  -6.99890000e-03   5.88800000e-01  -3.04830000e-01\n",
      "   2.39400000e-01  -2.25780000e-01  -1.88140000e-01   6.72660000e-01\n",
      "   4.21500000e-01   2.51850000e-01   1.87370000e-01   1.39160000e-01\n",
      "  -3.02590000e-01   1.38200000e-01  -1.10750000e+00  -1.17720000e-02\n",
      "   3.50860000e-04  -7.64520000e-02  -3.48650000e-02   1.56000000e-01\n",
      "  -3.23130000e-01   1.06920000e-01  -2.58070000e-02   3.03340000e-01\n",
      "  -3.88430000e-02   1.69700000e-01   3.41160000e-01   4.07090000e-01\n",
      "   6.54720000e-01   4.90000000e-01   5.19390000e-01   6.89400000e-02\n",
      "  -2.95500000e-01   2.23170000e-01   2.01500000e-01  -2.90730000e-01\n",
      "   3.04030000e-01   4.30910000e-01  -2.45070000e-01   3.77710000e-01\n",
      "  -7.82300000e-02   7.91140000e-02  -4.55180000e-01   7.34050000e-02\n",
      "  -8.76110000e-03  -2.59010000e-02  -9.54830000e-02   2.41170000e-01\n",
      "  -1.83610000e-01  -9.59110000e-01  -4.94920000e-01   1.18750000e-01\n",
      "   2.80810000e-01  -5.50430000e-02  -3.55830000e-01   1.88940000e-01\n",
      "  -2.37320000e-02   3.27470000e-02   6.01340000e-01  -4.20030000e-01\n",
      "   2.45600000e-01  -1.48680000e-01   8.81570000e-02  -3.37660000e-01\n",
      "   2.39760000e-01  -2.71690000e-01  -5.63710000e-01   1.99170000e-01\n",
      "  -1.19820000e-02  -3.96270000e-01   1.46430000e-01  -3.23390000e-01\n",
      "  -4.38830000e-01  -1.21490000e-01   6.20480000e-01  -2.56730000e-02\n",
      "  -3.08620000e-01  -3.10380000e-02   1.15970000e-01  -5.27510000e-02\n",
      "  -2.46630000e-01   3.53670000e-01   2.18490000e-01  -3.14860000e-01\n",
      "  -2.24290000e-01  -6.14340000e-01  -1.57220000e-01   9.39370000e-02\n",
      "  -3.26390000e-01   5.41620000e-01  -7.29700000e-02   2.04520000e-01\n",
      "  -2.75130000e-01   3.22860000e-01  -6.50280000e-01   4.53240000e-01\n",
      "  -2.68370000e-01  -1.39060000e-02   1.95180000e-01   1.49230000e-01\n",
      "   4.08240000e-01   4.52880000e-01  -7.89240000e-02   2.43910000e-01\n",
      "  -2.05690000e+00   5.24350000e-01   1.19360000e+00   3.92650000e-01\n",
      "  -8.28650000e-02  -5.14630000e-01   6.67980000e-03   7.30250000e-02\n",
      "   9.12750000e-02   3.20140000e-01   3.61690000e-01   7.01530000e-01\n",
      "   1.05940000e-01   2.39590000e-01  -7.23900000e-02  -7.70010000e-01\n",
      "  -8.74790000e-01   4.23260000e-01  -3.79820000e-01   9.45880000e-01\n",
      "  -4.24280000e-02   2.13510000e-02  -9.27650000e-02  -3.38130000e-01]\n"
     ]
    }
   ],
   "source": [
    "print glove_model['organization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047\n",
      "259\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for NLTK HMM Tagger\n",
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "doc_dir = 'Data/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print len(train_file_list)\n",
    "print len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory having tags\n",
    "tag_dir = 'Data/new_tags'\n",
    "\n",
    "tagEmbedding = dict()\n",
    "tagCount = dict()\n",
    "win_size = 2\n",
    "word_dim = 300\n",
    "unknown_words = set()\n",
    "\n",
    "for f in train_file_list:\n",
    "    #print 'Processing file '+f\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            for j in xrange(length):\n",
    "                # Get the word embedding of the word\n",
    "                word_embed = np.zeros(word_dim)\n",
    "                if (j>(win_size-1)) and (j<length-win_size):\n",
    "                    replace_unk = 'unk'\n",
    "                    for k in xrange(2*win_size + 1):\n",
    "                        if word_in_file[j-win_size+k].lower() in glove_model:\n",
    "                            word_embed = np.concatenate((word_embed, glove_model[word_in_file[j-win_size+k].lower()]), axis=0)\n",
    "                        else:\n",
    "                            unknown_words.add(word_in_file[j-win_size+k])\n",
    "                            #word_embed = np.concatenate((word_embed, glove_model['unk']), axis = 0)  \n",
    "                            word_embed = np.concatenate((word_embed, glove_model[replace_unk]), axis = 0)  \n",
    "                    # Create embdedding of Tag    \n",
    "                    if tag_in_file[j] in tagEmbedding:\n",
    "                        tagEmbedding[tag_in_file[j]] = np.add(tagEmbedding[tag_in_file[j]], word_embed)\n",
    "                        tagCount[tag_in_file[j]] = tagCount[tag_in_file[j]] + 1\n",
    "                    else:\n",
    "                        tagEmbedding[tag_in_file[j]] = word_embed\n",
    "                        tagCount[tag_in_file[j]] = 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tagCount:\n",
    "    tagEmbedding[key] = tagEmbedding[key]/tagCount[key]                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tagCount\n",
    "\n",
    "print '\\nPER_Assoc\\n'\n",
    "print tagEmbedding['PER_Assoc']\n",
    "print '\\nORG_Assoc\\n'\n",
    "print tagEmbedding['ORG_Assoc']\n",
    "print '\\n'\n",
    "print tagEmbedding['PER_Others']\n",
    "print '\\n'\n",
    "print tagEmbedding['Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Event']\n",
    "print '\\n'\n",
    "print tagEmbedding['O']\n",
    "print '\\n'\n",
    "print tagEmbedding['ORG_Accused']\n",
    "print '\\n'\n",
    "print tagEmbedding['PER_Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['ORG_Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Others']\n",
    "print '\\n'\n",
    "print tagEmbedding['ORG_Others']\n",
    "print '\\n'\n",
    "print tagEmbedding['PER_Accused']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Accused']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Assoc']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['Event']\n",
    "print '\\n'\n",
    "print tagEmbedding['Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev_077_st_027.txt\n",
      "2011_6_18_st-512.txt\n",
      "2010_10_18_st-192.txt\n",
      "ev_040_st_005.txt\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Create dictionary of words\n",
    "\n",
    "all_word_dict = dict()\n",
    "all_word_set = set()\n",
    "\n",
    "for f in test_file_list:\n",
    "    _word_file_path = os.path.join(doc_dir, f)\n",
    "    _lines_in_word_file = []\n",
    "    with open(_word_file_path, \"rt\") as _word_file:        \n",
    "        for _line in _word_file:\n",
    "            _lines_in_word_file.append(_line)\n",
    "        _num_lines = len(_lines_in_word_file)\n",
    "        for i in xrange(_num_lines):\n",
    "            _word_in_file = _lines_in_word_file[i].split()\n",
    "            _words_len = len(_word_in_file)\n",
    "            for j in xrange(_words_len):\n",
    "                all_word_set.add(_word_in_file[j])\n",
    "\n",
    "_word_list = list(all_word_set)\n",
    "for i in xrange(len(_word_list)):\n",
    "    all_word_dict[_word_list[i]] = i\n",
    "\n",
    "topN = 20\n",
    "Precision = dict()\n",
    "Recall = dict()\n",
    "num_docs_dont_have_tag = dict()\n",
    "\n",
    "Precision['PER_Others'] = 0\n",
    "Recall['PER_Others'] = 0\n",
    "Precision['PER_Victim'] = 0\n",
    "Recall['PER_Victim'] = 0\n",
    "Precision['PER_Accused'] = 0\n",
    "Recall['PER_Accused'] = 0\n",
    "Precision['ORG_Victim'] = 0\n",
    "Recall['ORG_Victim'] = 0\n",
    "Precision['ORG_Accused'] = 0\n",
    "Recall['ORG_Accused'] = 0\n",
    "Precision['ORG_Others'] = 0\n",
    "Recall['ORG_Others'] = 0\n",
    "Precision['LOC_Accused'] = 0\n",
    "Recall['LOC_Accused'] = 0\n",
    "Precision['LOC_Others'] = 0\n",
    "Recall['LOC_Others'] = 0\n",
    "Precision['LOC_Event'] = 0\n",
    "Recall['LOC_Event'] = 0\n",
    "Precision['LOC_Victim'] = 0\n",
    "Recall['LOC_Victim'] = 0\n",
    "Precision['PER_Assoc'] = 0\n",
    "Recall['PER_Assoc'] = 0\n",
    "Precision['ORG_Assoc'] = 0\n",
    "Recall['ORG_Assoc'] = 0\n",
    "Precision['Location'] = 0\n",
    "Recall['Location'] = 0\n",
    "Precision['Event'] = 0\n",
    "Recall['Event'] = 0\n",
    "Precision['LOC_Assoc'] = 0\n",
    "Recall['LOC_Assoc'] = 0\n",
    "Precision['O'] = 0\n",
    "Recall['O'] = 0\n",
    "Precision['Victim'] = 0\n",
    "Recall['Victim'] = 0\n",
    "Precision['Accused'] = 0\n",
    "Recall['Accused'] = 0\n",
    "\n",
    "num_docs_dont_have_tag['PER_Others'] = 0\n",
    "num_docs_dont_have_tag['PER_Victim'] = 0 \n",
    "num_docs_dont_have_tag['PER_Accused'] = 0    \n",
    "num_docs_dont_have_tag['ORG_Victim'] = 0\n",
    "num_docs_dont_have_tag['ORG_Accused'] = 0\n",
    "num_docs_dont_have_tag['ORG_Others'] = 0\n",
    "num_docs_dont_have_tag['LOC_Accused'] = 0\n",
    "num_docs_dont_have_tag['LOC_Others'] = 0\n",
    "num_docs_dont_have_tag['LOC_Event'] = 0\n",
    "num_docs_dont_have_tag['LOC_Victim'] = 0\n",
    "num_docs_dont_have_tag['PER_Assoc'] = 0\n",
    "num_docs_dont_have_tag['ORG_Assoc'] = 0\n",
    "num_docs_dont_have_tag['LOC_Assoc'] = 0\n",
    "num_docs_dont_have_tag['Event'] = 0\n",
    "num_docs_dont_have_tag['O'] = 0\n",
    "num_docs_dont_have_tag['Victim'] = 0\n",
    "num_docs_dont_have_tag['Location'] = 0\n",
    "num_docs_dont_have_tag['Accused'] = 0\n",
    "\n",
    "\n",
    "for f in test_file_list:\n",
    "    #print 'Processing file '+f\n",
    "    relevant = dict()\n",
    "    retrieved = dict()\n",
    "    sorted_retrieved = dict()\n",
    "    relevant['PER_Others'] = set()\n",
    "    retrieved['PER_Others'] = dict()\n",
    "    relevant['PER_Victim'] = set()\n",
    "    retrieved['PER_Victim'] = dict()\n",
    "    relevant['PER_Accused'] = set()\n",
    "    retrieved['PER_Accused'] = dict()\n",
    "    relevant['ORG_Victim'] = set()\n",
    "    retrieved['ORG_Victim'] = dict()\n",
    "    relevant['ORG_Accused'] = set()\n",
    "    retrieved['ORG_Accused'] = dict()\n",
    "    relevant['ORG_Others'] = set()\n",
    "    retrieved['ORG_Others'] = dict()\n",
    "    relevant['LOC_Accused'] = set()\n",
    "    retrieved['LOC_Accused'] = dict()\n",
    "    relevant['LOC_Others'] = set()\n",
    "    retrieved['LOC_Others'] = dict()\n",
    "    relevant['LOC_Event'] = set()\n",
    "    retrieved['LOC_Event'] = dict()\n",
    "    relevant['LOC_Victim'] = set()\n",
    "    retrieved['LOC_Victim'] = dict()    \n",
    "    relevant['PER_Assoc'] = set()\n",
    "    retrieved['PER_Assoc'] = dict()\n",
    "    relevant['ORG_Assoc'] = set()\n",
    "    retrieved['ORG_Assoc'] = dict()\n",
    "    relevant['LOC_Assoc'] = set()\n",
    "    retrieved['LOC_Assoc'] = dict()\n",
    "    relevant['Event'] = set()\n",
    "    retrieved['Event'] = dict()\n",
    "    relevant['Victim'] = set()\n",
    "    retrieved['Victim'] = dict()\n",
    "    relevant['Location'] = set()\n",
    "    retrieved['Location'] = dict()\n",
    "    relevant['O'] = set()\n",
    "    retrieved['O'] = dict()\n",
    "    relevant['Accused'] = set()\n",
    "    retrieved['Accused'] = dict()\n",
    "    \n",
    "    \n",
    "    sorted_retrieved['PER_Others'] = list()    \n",
    "    sorted_retrieved['PER_Victim'] = list()    \n",
    "    sorted_retrieved['PER_Accused'] = list()    \n",
    "    sorted_retrieved['ORG_Victim'] = list()    \n",
    "    sorted_retrieved['ORG_Accused'] = list()    \n",
    "    sorted_retrieved['ORG_Others'] = list()    \n",
    "    sorted_retrieved['LOC_Accused'] = list()    \n",
    "    sorted_retrieved['LOC_Others'] = list()    \n",
    "    sorted_retrieved['LOC_Event'] = list()\n",
    "    sorted_retrieved['LOC_Victim'] = list()    \n",
    "    sorted_retrieved['PER_Assoc'] = list()    \n",
    "    sorted_retrieved['ORG_Assoc'] = list()    \n",
    "    sorted_retrieved['LOC_Assoc'] = list()\n",
    "    sorted_retrieved['Event'] = list()\n",
    "    sorted_retrieved['O'] = list()\n",
    "    sorted_retrieved['Victim'] = list()\n",
    "    sorted_retrieved['Location'] = list()\n",
    "    sorted_retrieved['Accused'] = list()\n",
    "\n",
    "    \n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            \n",
    "            # This is number of all words which is N \n",
    "            length = min(len(word_in_file), len(tag_in_file))                \n",
    "            for j in xrange(length):\n",
    "                # Get the word embedding of the word\n",
    "                word_embed = np.zeros(word_dim)\n",
    "                \n",
    "                if (j>(win_size-1)) and (j<length-win_size):\n",
    "                    replace_unk = 'unk'\n",
    "                    for k in xrange(2*win_size + 1):\n",
    "                        if word_in_file[j-win_size+k].lower() in glove_model:\n",
    "                            word_embed = np.concatenate((word_embed, glove_model[word_in_file[j-win_size+k].lower()]), axis = 0)\n",
    "                        else:\n",
    "                            word_embed = np.concatenate((word_embed, glove_model['unk']), axis=0)  \n",
    "                    \n",
    "                    for tag in tagEmbedding:\n",
    "                        dist = 1 - (dot(tagEmbedding[tag], word_embed)/(norm(tagEmbedding[tag]) * norm(word_embed)))\n",
    "                        if word_in_file[j] in retrieved[tag].keys():\n",
    "                            retrieved[tag][word_in_file[j]] = retrieved[tag][word_in_file[j]]+dist\n",
    "                        else:\n",
    "                            retrieved[tag][word_in_file[j]] = dist\n",
    "                    \n",
    "                    relevant[tag_in_file[j]] =  word_in_file[j]\n",
    "   \n",
    "    # Sort the retrieved results by score\n",
    "    for tag in tagEmbedding:\n",
    "        sorted_retrieved[tag] = sorted(retrieved[tag].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #Calculate Precison and Recall for this file\n",
    "    try:\n",
    "        for tag in tagEmbedding:\n",
    "            tp = 0\n",
    "            for i in xrange(topN):\n",
    "                if sorted_retrieved[tag][i][0] in relevant[tag]:\n",
    "                    tp = tp+1\n",
    "            fp = topN-tp\n",
    "            fn = len(relevant[tag]) - tp\n",
    "            #print f+' '+tag+' '+str(tp)+' '+str(fp)+' '+str(fn)\n",
    "        \n",
    "            if len(relevant[tag]) == 0:\n",
    "                num_docs_dont_have_tag[tag] = num_docs_dont_have_tag[tag] +1\n",
    "            else:\n",
    "                precision = float((tp*100))/topN\n",
    "                recall = float((tp*100))/len(relevant[tag])\n",
    "            Precision[tag] = Precision[tag]+ precision\n",
    "            Recall[tag] = Recall[tag] + recall\n",
    "    except IndexError:\n",
    "        print f\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_Assoc 57.5386100386% 260.262503343% 94.2422292243\n",
      "ORG_Assoc 115.077220077% 505.644054305% 187.485477039\n",
      "PER_Others 6.18811881188% 23.6051215012% 9.80566698239\n",
      "Victim 138.888888889% 515.451986285% 218.817305679\n",
      "LOC_Event 6.08597285068% 22.1818540936% 9.55136467034\n",
      "Accused 101.923076923% 365.122932623% 159.360970814\n",
      "O 4.42084942085% 36.1043718187% 7.87716816159\n",
      "ORG_Accused 7.890625% 48.4471626464% 13.570940881\n",
      "PER_Victim 27.2368421053% 162.409483791% 46.6502205672\n",
      "ORG_Victim 39.8076923077% 238.443714405% 68.2253083848\n",
      "LOC_Others 5.66210045662% 21.3837430961% 8.95345721516\n",
      "ORG_Others 3.62790697674% 12.6190192237% 5.63560482968\n",
      "PER_Accused 11.8987341772% 44.6992072625% 18.7944639547\n",
      "LOC_Accused 18.9795918367% 67.3185941043% 29.6108064194\n",
      "LOC_Assoc 31.8333333333% 125.110870611% 50.7530185593\n",
      "LOC_Victim 79.5833333333% 312.972282347% 126.898592073\n",
      "Event 29.8333333333% 114.194203944% 47.3075332037\n",
      "Location 24.6052631579% 101.23832308% 39.5887570518\n"
     ]
    }
   ],
   "source": [
    "num_test_file = len(test_file_list)\n",
    "for tag in tagEmbedding:\n",
    "    precision = float(Precision[tag]/(num_test_file-num_docs_dont_have_tag[tag]))\n",
    "    recall = float(Recall[tag]/(num_test_file-num_docs_dont_have_tag[tag]))\n",
    "    f1 = float(2*precision*recall)/(precision+recall)\n",
    "    print tag, str(precision)+'%', str(recall)+'%', f1\n",
    "    #print(\"%.2f\", % precision)\n",
    "    #print(\"%.2f\", % recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
