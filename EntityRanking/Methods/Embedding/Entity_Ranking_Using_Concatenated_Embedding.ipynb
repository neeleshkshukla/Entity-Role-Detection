{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n",
      "[  3.00710000e-01  -4.68670000e-01  -2.06170000e-01  -8.09780000e-01\n",
      "  -2.38890000e-01   2.43290000e-01   1.65380000e-02  -3.56870000e-02\n",
      "  -2.23060000e-01   9.51890000e-01  -3.22730000e-01   2.19800000e-01\n",
      "  -6.75240000e-02  -3.72200000e-01  -3.97180000e-01  -4.38610000e-01\n",
      "   1.19670000e-01  -2.99640000e-01   2.84370000e-02  -8.75440000e-02\n",
      "   1.65690000e-01  -4.94510000e-01  -6.20110000e-01  -1.65740000e-01\n",
      "  -9.72180000e-02  -9.94740000e-02  -8.03070000e-02  -3.93380000e-01\n",
      "  -2.41950000e-01   3.20230000e-01  -5.33200000e-01  -4.01840000e-01\n",
      "  -6.71350000e-01  -7.85610000e-02   5.55460000e-01   2.99970000e-01\n",
      "  -9.96500000e-02  -6.70350000e-01   1.26690000e-01  -1.86180000e-01\n",
      "  -6.26210000e-02   4.52900000e-01   3.92650000e-01   2.41210000e-01\n",
      "  -4.14740000e-01  -6.18900000e-01  -1.04120000e-01  -3.10430000e-01\n",
      "  -6.67880000e-03  -8.32480000e-01   6.51500000e-01   9.01810000e-01\n",
      "   2.41460000e-02  -7.07660000e-02  -3.95800000e-01  -3.64870000e-01\n",
      "  -2.39290000e-01  -1.51450000e-01   2.07770000e-01   5.46710000e-01\n",
      "  -2.50420000e-01  -6.01420000e-01  -5.48200000e-01   7.72490000e-03\n",
      "  -5.32880000e-01   5.03250000e-01  -1.27120000e-01   1.19890000e-01\n",
      "  -6.45840000e-01   3.55760000e-01   1.74960000e-01   1.18380000e-01\n",
      "  -3.21810000e-01   7.48140000e-02  -9.03810000e-02  -2.98430000e-01\n",
      "   1.67980000e-02  -1.27350000e-01   7.35670000e-01  -1.73350000e-01\n",
      "   3.71230000e-01   3.79790000e-01  -5.18010000e-01   2.76210000e-01\n",
      "   2.15120000e-01  -8.25880000e-02   2.16380000e-01   1.25950000e-01\n",
      "   3.84360000e-01  -1.33320000e-01   5.71850000e-02  -2.81270000e-01\n",
      "  -4.43100000e-01   1.34980000e-01   1.33060000e-01  -3.20500000e-02\n",
      "   1.97190000e-01   2.54550000e-01   6.34750000e-01  -2.34740000e-01\n",
      "  -3.60380000e-01   4.11480000e-02  -2.44220000e-01   8.37310000e-01\n",
      "  -2.25040000e-01  -2.96830000e-01   6.38980000e-01  -3.97740000e-01\n",
      "  -1.03220000e-01  -1.74460000e-01  -7.80590000e-02   2.64790000e-01\n",
      "  -4.22500000e-01  -1.06710000e-01  -9.64680000e-02  -1.70270000e-01\n",
      "   2.74970000e-01  -1.28130000e-01   2.47510000e-01   2.59990000e-01\n",
      "   1.83270000e-01   1.09880000e-01   3.54860000e-04  -4.90290000e-01\n",
      "   1.95820000e-01  -4.52260000e-01  -1.36170000e-02   1.07650000e-01\n",
      "  -1.61610000e-02  -2.72420000e-01   7.78770000e-02  -1.18600000e-01\n",
      "   9.27920000e-02  -4.37740000e-01  -2.65390000e-01  -2.65900000e-01\n",
      "   8.05850000e-02   1.86260000e-01   1.73620000e-01  -2.02420000e-01\n",
      "   3.53270000e-01  -6.43350000e-02   1.37640000e-01  -4.44170000e-01\n",
      "   8.75210000e-01  -2.32600000e-01  -6.76570000e-01   2.38910000e-01\n",
      "  -8.01760000e-02   4.95260000e-01  -2.85790000e-01   2.50410000e-01\n",
      "   1.58530000e-01  -1.29600000e-01  -5.15290000e-01  -3.31750000e-01\n",
      "   4.18260000e-01   3.32110000e-01  -1.17930000e+00   2.28180000e-01\n",
      "  -5.77550000e-01   7.73140000e-01   1.60930000e-01   2.33600000e-01\n",
      "  -1.87640000e-01  -2.45160000e-01  -5.48030000e-01   2.31100000e-01\n",
      "  -3.29750000e-01  -1.26460000e-01   3.79840000e-01   3.60060000e-01\n",
      "   6.03820000e-01  -1.58820000e-01  -4.36820000e-01  -6.34440000e-01\n",
      "  -2.88300000e-01  -1.36090000e-01  -2.58210000e-02  -4.07670000e-01\n",
      "   1.86360000e-01  -4.58570000e-01  -2.46110000e-01  -4.58900000e-02\n",
      "   8.76130000e-02  -1.56850000e-01   3.01290000e-01  -8.01760000e-01\n",
      "   1.23630000e-01   7.14580000e-03   1.47510000e-01   3.54710000e-01\n",
      "   1.51200000e-01   8.19380000e-02  -3.67110000e-01  -2.72080000e-01\n",
      "  -3.55970000e-01   1.72070000e-01  -4.18500000e-02   4.95470000e-01\n",
      "  -3.16300000e-01   3.49150000e-01  -3.72950000e-02   2.09960000e-01\n",
      "  -3.01030000e-01  -1.08750000e-01   3.03540000e-01   2.81570000e-01\n",
      "  -7.98800000e-02  -5.06110000e-01  -2.94160000e-01  -3.08610000e-01\n",
      "  -8.24620000e-01  -1.00190000e-01   9.04730000e-02  -1.82380000e-01\n",
      "  -5.92230000e-03   6.38330000e-02   1.52100000e-01  -2.53850000e-01\n",
      "  -6.88310000e-01  -3.45490000e-02   4.51800000e-01   6.22930000e-02\n",
      "  -4.63430000e-01   4.04000000e-01   4.51060000e-02   1.73750000e-01\n",
      "  -2.77450000e-02   3.63610000e-01   7.82350000e-02   1.95380000e-01\n",
      "  -1.65060000e-01   3.96270000e-02  -3.01130000e-01   2.22570000e-01\n",
      "   2.67730000e-02   1.91510000e-01   4.99870000e-01  -3.54910000e-01\n",
      "  -1.99280000e-02   9.07010000e-01  -8.54900000e-01  -3.93610000e-01\n",
      "   4.10300000e-01   1.46310000e-01  -1.56640000e-01   5.39710000e-01\n",
      "   1.48690000e-01  -5.51930000e-02  -4.77180000e-01  -3.74980000e-01\n",
      "  -5.32330000e-01   1.23200000e-01  -1.22270000e-01  -5.81570000e-02\n",
      "   2.42450000e-02   1.25770000e-01  -1.85580000e-01   7.50540000e-02\n",
      "  -7.28220000e-01  -3.58780000e-02   1.59890000e-01  -2.57430000e-01\n",
      "   2.18560000e-01  -2.28100000e-01   4.83560000e-02   5.60230000e-02\n",
      "   1.51110000e-01   2.29450000e-01   4.58460000e-01  -8.70560000e-02\n",
      "  -2.96620000e-01   1.50540000e-02   2.11020000e-01  -3.74460000e-02\n",
      "   7.89020000e-01  -3.41930000e-01  -8.24300000e-01  -7.17480000e-01\n",
      "  -1.96490000e-01   8.75700000e-02  -2.05520000e-01   1.46100000e-02\n",
      "  -3.80880000e-01   6.53090000e-01  -2.05610000e-01  -2.84270000e-02\n",
      "   1.05770000e-02   8.84100000e-03   1.19920000e-01   1.46110000e-01\n",
      "   1.60340000e-01   7.24310000e-02  -4.37600000e-01  -2.59790000e-01\n",
      "   5.81580000e-01   4.92670000e-01  -1.12760000e-01  -2.77750000e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load Glove Embedding\n",
    "gloveFile = \"glove.6B/glove.6B.300d.txt\"\n",
    "glove_model = {}\n",
    "\n",
    "with open(gloveFile, 'r') as f:\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        glove_model[word] = embedding\n",
    "    print \"Done.\",len(glove_model),\" words loaded!\"  \n",
    "\n",
    "print glove_model['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.57080000e-02  -4.08090000e-03  -1.71850000e-01   1.57380000e-01\n",
      "  -1.54510000e-01   1.29430000e-01  -3.71710000e-01   2.87720000e-01\n",
      "  -1.77560000e-01  -1.74180000e+00   6.77970000e-01   1.95320000e-01\n",
      "  -2.10470000e-01   3.12780000e-02  -2.72330000e-01  -9.39930000e-02\n",
      "   2.69210000e-01   3.84920000e-01  -5.40090000e-01  -1.63370000e-01\n",
      "   3.35600000e-01   4.30180000e-01   5.00560000e-02   1.74400000e-02\n",
      "  -5.07370000e-02   1.41920000e-01   1.30980000e-01   1.13790000e-01\n",
      "  -3.73490000e-01   2.58930000e-01   3.53490000e-01   1.49580000e-01\n",
      "  -5.85640000e-03  -2.07550000e-01  -1.93410000e-01   4.63010000e-02\n",
      "   3.92770000e-01  -2.71260000e-02  -3.01780000e-01  -5.89790000e-01\n",
      "  -2.54810000e-02   1.23350000e-01   4.40330000e-01   3.42790000e-01\n",
      "  -1.65740000e-01  -2.59720000e-01  -3.33980000e-01   4.92420000e-02\n",
      "  -6.27630000e-02  -3.90970000e-01  -7.25190000e-02   2.46290000e-01\n",
      "   5.15210000e-02   4.74460000e-02  -3.80240000e-01  -1.71270000e-01\n",
      "  -3.02750000e-01   7.29570000e-01  -4.55880000e-02  -2.32660000e-01\n",
      "   4.45110000e-02   3.68980000e-01   6.87840000e-02   3.07620000e-02\n",
      "  -5.92710000e-01   9.16230000e-02   1.97050000e-02   2.32800000e-01\n",
      "  -2.89010000e-01   2.95540000e-01  -2.17190000e-01   3.45850000e-01\n",
      "   2.34270000e-01   2.80520000e-01  -2.03050000e-01  -2.52690000e-01\n",
      "   3.28420000e-01  -3.79880000e-01  -4.57610000e-02   5.41780000e-01\n",
      "  -3.25700000e-01   1.88750000e-01  -4.57980000e-01   1.35760000e-01\n",
      "  -2.60850000e-01   1.53620000e-01  -1.37920000e-01   1.04600000e-01\n",
      "  -3.41910000e-01   9.95600000e-02  -5.09810000e-01   1.28410000e-01\n",
      "  -2.09620000e-01   2.67890000e-01  -4.67400000e-02  -5.08580000e-02\n",
      "  -1.12890000e-01   4.60590000e-02  -1.51910000e-01   2.41750000e-01\n",
      "   2.45110000e-01  -3.98710000e-01   4.19060000e-01  -4.67660000e-01\n",
      "   2.58460000e-01  -1.71010000e-01   3.10260000e-01  -1.95030000e-01\n",
      "   3.43410000e-01   4.87730000e-01  -1.78470000e-01  -3.00120000e-01\n",
      "  -1.88740000e-01   4.61740000e-01   1.24120000e-01  -4.13390000e-01\n",
      "  -1.37010000e-01  -1.35040000e-01   5.95200000e-02   4.34570000e-01\n",
      "  -5.69670000e-02   4.09090000e-01   6.75980000e-01   2.47790000e-01\n",
      "   2.95890000e-01  -1.52300000e-02   3.12670000e-01   1.92370000e-01\n",
      "   8.01340000e-02  -1.81910000e-01  -7.09580000e-02  -5.72070000e-01\n",
      "  -2.34760000e-02  -7.93620000e-01  -2.67930000e-02  -7.60910000e-02\n",
      "  -2.77050000e-01   3.10950000e-01  -5.65220000e-01  -2.89820000e-01\n",
      "  -1.45660000e-01   1.45520000e-01  -1.26980000e-01  -4.62210000e-01\n",
      "   2.28810000e-01   5.26260000e-01   5.65560000e-02   3.74050000e-02\n",
      "  -2.80090000e-01  -3.02720000e-03   6.69990000e-01  -4.38630000e-01\n",
      "  -5.70310000e-03  -3.58170000e-02  -3.78680000e-01   4.72010000e-01\n",
      "   3.54650000e-01  -1.26870000e-01  -6.94010000e-02  -2.17930000e-01\n",
      "  -2.75310000e-01   7.60700000e-01  -2.33760000e-01  -1.47260000e-01\n",
      "   4.41780000e-02  -8.71570000e-01  -3.06890000e-01   2.37990000e-01\n",
      "  -3.12740000e-01   5.95630000e-01  -2.59220000e-01  -4.49890000e-01\n",
      "  -1.02610000e+00  -6.99890000e-03   5.88800000e-01  -3.04830000e-01\n",
      "   2.39400000e-01  -2.25780000e-01  -1.88140000e-01   6.72660000e-01\n",
      "   4.21500000e-01   2.51850000e-01   1.87370000e-01   1.39160000e-01\n",
      "  -3.02590000e-01   1.38200000e-01  -1.10750000e+00  -1.17720000e-02\n",
      "   3.50860000e-04  -7.64520000e-02  -3.48650000e-02   1.56000000e-01\n",
      "  -3.23130000e-01   1.06920000e-01  -2.58070000e-02   3.03340000e-01\n",
      "  -3.88430000e-02   1.69700000e-01   3.41160000e-01   4.07090000e-01\n",
      "   6.54720000e-01   4.90000000e-01   5.19390000e-01   6.89400000e-02\n",
      "  -2.95500000e-01   2.23170000e-01   2.01500000e-01  -2.90730000e-01\n",
      "   3.04030000e-01   4.30910000e-01  -2.45070000e-01   3.77710000e-01\n",
      "  -7.82300000e-02   7.91140000e-02  -4.55180000e-01   7.34050000e-02\n",
      "  -8.76110000e-03  -2.59010000e-02  -9.54830000e-02   2.41170000e-01\n",
      "  -1.83610000e-01  -9.59110000e-01  -4.94920000e-01   1.18750000e-01\n",
      "   2.80810000e-01  -5.50430000e-02  -3.55830000e-01   1.88940000e-01\n",
      "  -2.37320000e-02   3.27470000e-02   6.01340000e-01  -4.20030000e-01\n",
      "   2.45600000e-01  -1.48680000e-01   8.81570000e-02  -3.37660000e-01\n",
      "   2.39760000e-01  -2.71690000e-01  -5.63710000e-01   1.99170000e-01\n",
      "  -1.19820000e-02  -3.96270000e-01   1.46430000e-01  -3.23390000e-01\n",
      "  -4.38830000e-01  -1.21490000e-01   6.20480000e-01  -2.56730000e-02\n",
      "  -3.08620000e-01  -3.10380000e-02   1.15970000e-01  -5.27510000e-02\n",
      "  -2.46630000e-01   3.53670000e-01   2.18490000e-01  -3.14860000e-01\n",
      "  -2.24290000e-01  -6.14340000e-01  -1.57220000e-01   9.39370000e-02\n",
      "  -3.26390000e-01   5.41620000e-01  -7.29700000e-02   2.04520000e-01\n",
      "  -2.75130000e-01   3.22860000e-01  -6.50280000e-01   4.53240000e-01\n",
      "  -2.68370000e-01  -1.39060000e-02   1.95180000e-01   1.49230000e-01\n",
      "   4.08240000e-01   4.52880000e-01  -7.89240000e-02   2.43910000e-01\n",
      "  -2.05690000e+00   5.24350000e-01   1.19360000e+00   3.92650000e-01\n",
      "  -8.28650000e-02  -5.14630000e-01   6.67980000e-03   7.30250000e-02\n",
      "   9.12750000e-02   3.20140000e-01   3.61690000e-01   7.01530000e-01\n",
      "   1.05940000e-01   2.39590000e-01  -7.23900000e-02  -7.70010000e-01\n",
      "  -8.74790000e-01   4.23260000e-01  -3.79820000e-01   9.45880000e-01\n",
      "  -4.24280000e-02   2.13510000e-02  -9.27650000e-02  -3.38130000e-01]\n"
     ]
    }
   ],
   "source": [
    "print glove_model['organization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for NLTK HMM Tagger\n",
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "doc_dir = 'Data/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print len(train_file_list)\n",
    "print len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory having tags\n",
    "tag_dir = 'Data/new_tags'\n",
    "\n",
    "tagEmbedding = dict()\n",
    "tagCount = dict()\n",
    "win_size = 2\n",
    "word_dim = 300\n",
    "unknown_words = set()\n",
    "\n",
    "for f in train_file_list:\n",
    "    #print 'Processing file '+f\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            for j in xrange(length):\n",
    "                # Get the word embedding of the word\n",
    "                word_embed = np.zeros(word_dim)\n",
    "                if (j>(win_size-1)) and (j<length-win_size):\n",
    "                    replace_unk = 'unk'\n",
    "                    for k in xrange(2*win_size + 1):\n",
    "                        if word_in_file[j-win_size+k].lower() in glove_model:\n",
    "                            word_embed = np.concatenate((word_embed, glove_model[word_in_file[j-win_size+k].lower()]), axis=0)\n",
    "                        else:\n",
    "                            unknown_words.add(word_in_file[j-win_size+k])\n",
    "                            #word_embed = np.concatenate((word_embed, glove_model['unk']), axis = 0)  \n",
    "                            word_embed = np.concatenate((word_embed, glove_model[replace_unk]), axis = 0)  \n",
    "                    # Create embdedding of Tag    \n",
    "                    if tag_in_file[j] in tagEmbedding:\n",
    "                        tagEmbedding[tag_in_file[j]] = np.add(tagEmbedding[tag_in_file[j]], word_embed)\n",
    "                        tagCount[tag_in_file[j]] = tagCount[tag_in_file[j]] + 1\n",
    "                    else:\n",
    "                        tagEmbedding[tag_in_file[j]] = word_embed\n",
    "                        tagCount[tag_in_file[j]] = 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tagCount:\n",
    "    tagEmbedding[key] = tagEmbedding[key]/tagCount[key]                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER_Assoc': 168, 'ORG_Assoc': 51, 'PER_Others': 7114, 'Victim': 27, 'LOC_Event': 5346, 'Accused': 192, 'O': 351447, 'ORG_Accused': 2427, 'PER_Victim': 1066, 'ORG_Victim': 321, 'LOC_Others': 7178, 'ORG_Others': 9286, 'PER_Accused': 3627, 'LOC_Accused': 687, 'LOC_Assoc': 314, 'LOC_Victim': 194, 'Event': 313, 'Location': 544}\n",
      "\n",
      "PER_Assoc\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.09482873 -0.20728944\n",
      "  0.03050485]\n",
      "\n",
      "ORG_Assoc\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.10847407 -0.18233476\n",
      " -0.01068716]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.12191061 -0.26938584\n",
      "  0.14030579]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.22678321 -0.17522766\n",
      "  0.00934241]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.16698668 -0.17334857\n",
      "  0.08105636]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.16366825 -0.18297412\n",
      "  0.03669295]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.09672384 -0.1517116\n",
      " -0.09163813]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.1731721  -0.18890204\n",
      "  0.07259167]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.18377903 -0.19534634\n",
      "  0.10613773]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.19228136 -0.15740363\n",
      "  0.0831521 ]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.1736571  -0.185841    0.03745636]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.12242502 -0.24119972\n",
      "  0.07248651]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.18138629 -0.15600398\n",
      "  0.07089614]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.1907095  -0.20827173\n",
      "  0.0635497 ]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.15076913 -0.12435577\n",
      "  0.10042004]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.18213707 -0.21593896\n",
      "  0.03319805]\n",
      "\n",
      "\n",
      "[ 0.          0.          0.         ..., -0.16944237 -0.19205171\n",
      "  0.03283076]\n"
     ]
    }
   ],
   "source": [
    "print tagCount\n",
    "\n",
    "print '\\nPER_Assoc\\n'\n",
    "print tagEmbedding['PER_Assoc']\n",
    "print '\\nORG_Assoc\\n'\n",
    "print tagEmbedding['ORG_Assoc']\n",
    "print '\\n'\n",
    "print tagEmbedding['PER_Others']\n",
    "print '\\n'\n",
    "print tagEmbedding['Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Event']\n",
    "print '\\n'\n",
    "print tagEmbedding['O']\n",
    "print '\\n'\n",
    "print tagEmbedding['ORG_Accused']\n",
    "print '\\n'\n",
    "print tagEmbedding['PER_Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['ORG_Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Others']\n",
    "print '\\n'\n",
    "print tagEmbedding['ORG_Others']\n",
    "print '\\n'\n",
    "print tagEmbedding['PER_Accused']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Accused']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Assoc']\n",
    "print '\\n'\n",
    "print tagEmbedding['LOC_Victim']\n",
    "print '\\n'\n",
    "print tagEmbedding['Event']\n",
    "print '\\n'\n",
    "print tagEmbedding['Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 2012_6_11_st-43.txt\n",
      "Processing file ev_051_st_007.txt\n",
      "Processing file ev_029_st_001.txt\n",
      "Processing file 2012_8_2_st-724.txt\n",
      "Processing file ev_031_st_001.txt\n",
      "Processing file ev_087_st_016.txt\n",
      "Processing file ev_031_st_003.txt\n",
      "Processing file 2011_9_14_st-2.txt\n",
      "Processing file 2011_7_23_st-1.txt\n",
      "Processing file ev_032_st_002.txt\n",
      "Processing file ev_094_st_004.txt\n",
      "Processing file 2012_8_6_st-1.txt\n",
      "Processing file 2013_5_8_st-59.txt\n",
      "Processing file ev_092_st_015.txt\n",
      "Processing file 2011_3_10_st-144.txt\n",
      "Processing file 2010_9_4_st-331.txt\n",
      "Processing file ev_075_st_002.txt\n",
      "Processing file 2011_8_16_st-674.txt\n",
      "Processing file ev_074_st_001.txt\n",
      "Processing file 2011_9_23_st-45.txt\n",
      "Processing file 2011_7_17_st-56.txt\n",
      "Processing file 2012_5_3_st-53.txt\n",
      "Processing file 2013_3_2_st-40.txt\n",
      "Processing file ev_006_st_001.txt\n",
      "Processing file ev_042_st_006.txt\n",
      "Processing file ev_081_st_011.txt\n",
      "Processing file ev_093_st_026.txt\n",
      "Processing file ev_075_st_009.txt\n",
      "Processing file 2012_9_6_st-825.txt\n",
      "Processing file 2011_12_1_st-1.txt\n",
      "Processing file ev_068_st_010.txt\n",
      "Processing file 2010_7_20_st-558.txt\n",
      "Processing file 2011_9_1_st-670.txt\n",
      "Processing file 2014_12_1_st-6002.txt\n",
      "Processing file ev_002_st_001.txt\n",
      "Processing file 2013_3_21_st-17.txt\n",
      "Processing file ev_002_st_002.txt\n",
      "Processing file ev_063_st_008.txt\n",
      "Processing file 2012_9_24_st-37.txt\n",
      "Processing file ev_086_st_032.txt\n",
      "Processing file ev_073_st_007.txt\n",
      "Processing file 2012_2_17_st-6.txt\n",
      "Processing file 2011_7_17_st-1.txt\n",
      "Processing file 2013_8_29_st-745.txt\n",
      "Processing file ev_072_st_003.txt\n",
      "Processing file 2013_6_13_st-397.txt\n",
      "Processing file 2013_5_12_st-13.txt\n",
      "Processing file ev_076_st_021.txt\n",
      "Processing file ev_001_st_007.txt\n",
      "Processing file ev_089_st_006.txt\n",
      "Processing file 2010_11_12_st-43.txt\n",
      "Processing file ev_063_st_009.txt\n",
      "Processing file 2010_11_4_st-8.txt\n",
      "Processing file ev_081_st_004.txt\n",
      "Processing file 2012_11_5_st-679.txt\n",
      "Processing file ev_089_st_020.txt\n",
      "Processing file 2010_4_17_st-395.txt\n",
      "Processing file ev_066_st_018.txt\n",
      "Processing file ev_019_st_031.txt\n",
      "Processing file ev_019_st_027.txt\n",
      "Processing file 2012_1_12_st-17.txt\n",
      "Processing file ev_076_st_024.txt\n",
      "Processing file ev_065_st_011.txt\n",
      "Processing file ev_060_st_002.txt\n",
      "Processing file ev_094_st_025.txt\n",
      "Processing file 2013_6_10_st-287.txt\n",
      "Processing file 2014_10_6_st-4004.txt\n",
      "Processing file ev_059_st_004.txt\n",
      "Processing file ev_075_st_017.txt\n",
      "Processing file ev_083_st_015.txt\n",
      "Processing file ev_070_st_006.txt\n",
      "Processing file 2013_9_30_st-7.txt\n",
      "Processing file ev_058_st_001.txt\n",
      "Processing file ev_014_st_015.txt\n",
      "Processing file 2010_5_2_st-373.txt\n",
      "Processing file 2012_4_25_st-76.txt\n",
      "Processing file 2013_6_4_st-601.txt\n",
      "Processing file 2013_8_31_st-139.txt\n",
      "Processing file 2012_3_6_st-74.txt\n",
      "Processing file ev_074_st_015.txt\n",
      "Processing file ev_075_st_014.txt\n",
      "Processing file 2012_2_24_st-585.txt\n",
      "Processing file ev_039_st_014.txt\n",
      "Processing file 2010_3_2_st-12.txt\n",
      "Processing file ev_080_st_003.txt\n",
      "Processing file ev_037_st_007.txt\n",
      "Processing file ev_019_st_025.txt\n",
      "Processing file ev_079_st_016.txt\n",
      "Processing file ev_029_st_006.txt\n",
      "Processing file ev_019_st_007.txt\n",
      "Processing file 2011_10_30_st-99.txt\n",
      "Processing file 2010_12_2_st-81.txt\n",
      "Processing file ev_019_st_010.txt\n",
      "Processing file ev_063_st_007.txt\n",
      "Processing file 2013_8_30_st-10.txt\n",
      "Processing file 2010_9_2_st-663.txt\n",
      "Processing file ev_063_st_003.txt\n",
      "Processing file 2011_5_28_st-68.txt\n",
      "Processing file ev_081_st_009.txt\n",
      "Processing file ev_087_st_005.txt\n",
      "Processing file ev_087_st_027.txt\n",
      "Processing file 2010_7_3_st-605.txt\n",
      "Processing file ev_061_st_014.txt\n",
      "Processing file 2010_9_20_st-581.txt\n",
      "Processing file ev_094_st_021.txt\n",
      "Processing file 2011_7_15_st-12.txt\n",
      "Processing file 2010_8_25_st-33.txt\n",
      "Processing file ev_078_st_004.txt\n",
      "Processing file 2014_11_27_st-6001.txt\n",
      "Processing file ev_084_st_007.txt\n",
      "Processing file 2010_11_29_st-28.txt\n",
      "Processing file ev_025_st_004.txt\n",
      "Processing file ev_079_st_003.txt\n",
      "Processing file 2011_10_17_st-601.txt\n",
      "Processing file ev_094_st_022.txt\n",
      "Processing file ev_084_st_017.txt\n",
      "Processing file 2011_9_9_st-48.txt\n",
      "Processing file ev_076_st_007.txt\n",
      "Processing file ev_076_st_003.txt\n",
      "Processing file ev_094_st_012.txt\n",
      "Processing file 2012_1_21_st-38.txt\n",
      "Processing file ev_034_st_003.txt\n",
      "Processing file ev_061_st_007.txt\n",
      "Processing file ev_084_st_013.txt\n",
      "Processing file ev_062_st_007.txt\n",
      "Processing file ev_075_st_007.txt\n",
      "Processing file ev_088_st_001.txt\n",
      "Processing file ev_084_st_025.txt\n",
      "Processing file ev_058_st_002.txt\n",
      "Processing file ev_089_st_029.txt\n",
      "Processing file 2013_6_16_st-42.txt\n",
      "Processing file ev_039_st_004.txt\n",
      "Processing file ev_082_st_016.txt\n",
      "Processing file ev_084_st_016.txt\n",
      "Processing file ev_085_st_003.txt\n",
      "Processing file ev_072_st_007.txt\n",
      "Processing file ev_014_st_012.txt\n",
      "Processing file 2010_3_30_st-27.txt\n",
      "Processing file 2012_5_30_st-606.txt\n",
      "Processing file ev_050_st_001.txt\n",
      "Processing file ev_074_st_016.txt\n",
      "Processing file ev_065_st_002.txt\n",
      "Processing file ev_049_st_003.txt\n",
      "Processing file ev_019_st_019.txt\n",
      "Processing file ev_035_st_009.txt\n",
      "Processing file ev_072_st_002.txt\n",
      "Processing file 2013_9_6_st-763.txt\n",
      "Processing file ev_004_st_002.txt\n",
      "Processing file 2012_2_8_st-48.txt\n",
      "Processing file 2010_8_21_st-267.txt\n",
      "Processing file ev_089_st_017.txt\n",
      "Processing file 2013_5_26_st-561.txt\n",
      "Processing file ev_083_st_034.txt\n",
      "Processing file ev_055_st_001.txt\n",
      "Processing file 2013_3_8_st-793.txt\n",
      "Processing file ev_077_st_024.txt\n",
      "Processing file 2011_12_8_st-694.txt\n",
      "Processing file ev_089_st_038.txt\n",
      "Processing file ev_075_st_003.txt\n",
      "Processing file 2012_6_28_st-97.txt\n",
      "Processing file ev_005_st_001.txt\n",
      "Processing file ev_074_st_010.txt\n",
      "Processing file 2013_4_19_st-1091.txt\n",
      "Processing file 2010_7_1_st-312.txt\n",
      "Processing file 2013_2_22_st-438.txt\n",
      "Processing file ev_066_st_001.txt\n",
      "Processing file 2013_2_22_st-425.txt\n",
      "Processing file ev_005_st_003.txt\n",
      "Processing file ev_070_st_001.txt\n",
      "Processing file ev_037_st_006.txt\n",
      "Processing file 2011_1_11_st-624.txt\n",
      "Processing file ev_019_st_011.txt\n",
      "Processing file 2013_4_3_st-489.txt\n",
      "Processing file ev_093_st_014.txt\n",
      "Processing file 2010_12_22_st-19.txt\n",
      "Processing file 2013_5_12_st-41.txt\n",
      "Processing file 2011_2_11_st-63.txt\n",
      "Processing file 2013_4_17_st-15.txt\n",
      "Processing file ev_035_st_017.txt\n",
      "Processing file 2010_10_19_st-227.txt\n",
      "Processing file ev_068_st_003.txt\n",
      "Processing file 2013_4_24_st-79.txt\n",
      "Processing file ev_079_st_007.txt\n",
      "Processing file ev_087_st_010.txt\n",
      "Processing file ev_019_st_037.txt\n",
      "Processing file ev_019_st_005.txt\n",
      "Processing file ev_086_st_019.txt\n",
      "Processing file ev_016_st_003.txt\n",
      "Processing file ev_075_st_019.txt\n",
      "Processing file 2011_11_15_st-695.txt\n",
      "Processing file 2010_5_20_st-370.txt\n",
      "Processing file 2013_9_20_st-56.txt\n",
      "Processing file ev_067_st_003.txt\n",
      "Processing file 2010_12_10_st-64.txt\n",
      "Processing file 2013_4_23_st-14.txt\n",
      "Processing file ev_019_st_024.txt\n",
      "Processing file ev_088_st_003.txt\n",
      "Processing file 2013_4_2_st-5.txt\n",
      "Processing file ev_076_st_010.txt\n",
      "Processing file ev_092_st_004.txt\n",
      "Processing file 2011_9_8_st-4.txt\n",
      "Processing file 2011_8_13_st-46.txt\n",
      "Processing file ev_076_st_018.txt\n",
      "Processing file ev_091_st_006.txt\n",
      "Processing file 2011_3_17_st-317.txt\n",
      "Processing file ev_063_st_001.txt\n",
      "Processing file 2011_7_15_st-11.txt\n",
      "Processing file ev_089_st_025.txt\n",
      "Processing file ev_037_st_003.txt\n",
      "Processing file 2013_4_18_st-47.txt\n",
      "Processing file ev_054_st_001.txt\n",
      "Processing file ev_093_st_008.txt\n",
      "Processing file ev_090_st_007.txt\n",
      "Processing file ev_092_st_008.txt\n",
      "Processing file 2011_6_6_st-15.txt\n",
      "Processing file ev_019_st_039.txt\n",
      "Processing file 2013_9_1_st-97.txt\n",
      "Processing file ev_001_st_001.txt\n",
      "Processing file 2013_7_10_st-155.txt\n",
      "Processing file ev_019_st_009.txt\n",
      "Processing file ev_039_st_009.txt\n",
      "Processing file ev_035_st_006.txt\n",
      "Processing file 2013_9_12_st-50.txt\n",
      "Processing file ev_088_st_011.txt\n",
      "Processing file 2010_6_5_st-12.txt\n",
      "Processing file ev_040_st_005.txt\n",
      "Processing file ev_088_st_006.txt\n",
      "Processing file ev_082_st_004.txt\n",
      "Processing file ev_061_st_009.txt\n",
      "Processing file ev_085_st_011.txt\n",
      "Processing file ev_060_st_004.txt\n",
      "Processing file ev_076_st_017.txt\n",
      "Processing file ev_086_st_005.txt\n",
      "Processing file ev_077_st_005.txt\n",
      "Processing file ev_037_st_001.txt\n",
      "Processing file ev_065_st_001.txt\n",
      "Processing file 2011_1_21_st-764.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ev_075_st_024.txt\n",
      "Processing file ev_089_st_003.txt\n",
      "Processing file 2014_11_12_st-4002.txt\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Create dictionary of words\n",
    "\n",
    "all_word_dict = dict()\n",
    "all_word_set = set()\n",
    "\n",
    "for f in test_file_list:\n",
    "    _word_file_path = os.path.join(doc_dir, f)\n",
    "    _lines_in_word_file = []\n",
    "    with open(_word_file_path, \"rt\") as _word_file:        \n",
    "        for _line in _word_file:\n",
    "            _lines_in_word_file.append(_line)\n",
    "        _num_lines = len(_lines_in_word_file)\n",
    "        for i in xrange(_num_lines):\n",
    "            _word_in_file = _lines_in_word_file[i].split()\n",
    "            _words_len = len(_word_in_file)\n",
    "            for j in xrange(_words_len):\n",
    "                all_word_set.add(_word_in_file[j])\n",
    "\n",
    "_word_list = list(all_word_set)\n",
    "for i in xrange(len(_word_list)):\n",
    "    all_word_dict[_word_list[i]] = i\n",
    "\n",
    "topN = 5\n",
    "Precision = dict()\n",
    "Recall = dict()\n",
    "num_docs_dont_have_tag = dict()\n",
    "\n",
    "Precision['PER_Others'] = 0\n",
    "Recall['PER_Others'] = 0\n",
    "Precision['PER_Victim'] = 0\n",
    "Recall['PER_Victim'] = 0\n",
    "Precision['PER_Accused'] = 0\n",
    "Recall['PER_Accused'] = 0\n",
    "Precision['ORG_Victim'] = 0\n",
    "Recall['ORG_Victim'] = 0\n",
    "Precision['ORG_Accused'] = 0\n",
    "Recall['ORG_Accused'] = 0\n",
    "Precision['ORG_Others'] = 0\n",
    "Recall['ORG_Others'] = 0\n",
    "Precision['LOC_Accused'] = 0\n",
    "Recall['LOC_Accused'] = 0\n",
    "Precision['LOC_Others'] = 0\n",
    "Recall['LOC_Others'] = 0\n",
    "Precision['LOC_Event'] = 0\n",
    "Recall['LOC_Event'] = 0\n",
    "Precision['LOC_Victim'] = 0\n",
    "Recall['LOC_Victim'] = 0\n",
    "Precision['PER_Assoc'] = 0\n",
    "Recall['PER_Assoc'] = 0\n",
    "Precision['ORG_Assoc'] = 0\n",
    "Recall['ORG_Assoc'] = 0\n",
    "Precision['Location'] = 0\n",
    "Recall['Location'] = 0\n",
    "Precision['Event'] = 0\n",
    "Recall['Event'] = 0\n",
    "Precision['LOC_Assoc'] = 0\n",
    "Recall['LOC_Assoc'] = 0\n",
    "Precision['O'] = 0\n",
    "Recall['O'] = 0\n",
    "Precision['Victim'] = 0\n",
    "Recall['Victim'] = 0\n",
    "Precision['Accused'] = 0\n",
    "Recall['Accused'] = 0\n",
    "\n",
    "num_docs_dont_have_tag['PER_Others'] = 0\n",
    "num_docs_dont_have_tag['PER_Victim'] = 0 \n",
    "num_docs_dont_have_tag['PER_Accused'] = 0    \n",
    "num_docs_dont_have_tag['ORG_Victim'] = 0\n",
    "num_docs_dont_have_tag['ORG_Accused'] = 0\n",
    "num_docs_dont_have_tag['ORG_Others'] = 0\n",
    "num_docs_dont_have_tag['LOC_Accused'] = 0\n",
    "num_docs_dont_have_tag['LOC_Others'] = 0\n",
    "num_docs_dont_have_tag['LOC_Event'] = 0\n",
    "num_docs_dont_have_tag['LOC_Victim'] = 0\n",
    "num_docs_dont_have_tag['PER_Assoc'] = 0\n",
    "num_docs_dont_have_tag['ORG_Assoc'] = 0\n",
    "num_docs_dont_have_tag['LOC_Assoc'] = 0\n",
    "num_docs_dont_have_tag['Event'] = 0\n",
    "num_docs_dont_have_tag['O'] = 0\n",
    "num_docs_dont_have_tag['Victim'] = 0\n",
    "num_docs_dont_have_tag['Location'] = 0\n",
    "num_docs_dont_have_tag['Accused'] = 0\n",
    "\n",
    "\n",
    "for f in test_file_list:\n",
    "    print 'Processing file '+f\n",
    "    relevant = dict()\n",
    "    retrieved = dict()\n",
    "    sorted_retrieved = dict()\n",
    "    retrieved_count = dict()\n",
    "    relevant['PER_Others'] = set()\n",
    "    retrieved['PER_Others'] = dict()\n",
    "    relevant['PER_Victim'] = set()\n",
    "    retrieved['PER_Victim'] = dict()\n",
    "    relevant['PER_Accused'] = set()\n",
    "    retrieved['PER_Accused'] = dict()\n",
    "    relevant['ORG_Victim'] = set()\n",
    "    retrieved['ORG_Victim'] = dict()\n",
    "    relevant['ORG_Accused'] = set()\n",
    "    retrieved['ORG_Accused'] = dict()\n",
    "    relevant['ORG_Others'] = set()\n",
    "    retrieved['ORG_Others'] = dict()\n",
    "    relevant['LOC_Accused'] = set()\n",
    "    retrieved['LOC_Accused'] = dict()\n",
    "    relevant['LOC_Others'] = set()\n",
    "    retrieved['LOC_Others'] = dict()\n",
    "    relevant['LOC_Event'] = set()\n",
    "    retrieved['LOC_Event'] = dict()\n",
    "    relevant['LOC_Victim'] = set()\n",
    "    retrieved['LOC_Victim'] = dict()    \n",
    "    relevant['PER_Assoc'] = set()\n",
    "    retrieved['PER_Assoc'] = dict()\n",
    "    relevant['ORG_Assoc'] = set()\n",
    "    retrieved['ORG_Assoc'] = dict()\n",
    "    relevant['LOC_Assoc'] = set()\n",
    "    retrieved['LOC_Assoc'] = dict()\n",
    "    relevant['Event'] = set()\n",
    "    retrieved['Event'] = dict()\n",
    "    relevant['Victim'] = set()\n",
    "    retrieved['Victim'] = dict()\n",
    "    relevant['Location'] = set()\n",
    "    retrieved['Location'] = dict()\n",
    "    relevant['O'] = set()\n",
    "    retrieved['O'] = dict()\n",
    "    relevant['Accused'] = set()\n",
    "    retrieved['Accused'] = dict()\n",
    "    \n",
    "    \n",
    "    sorted_retrieved['PER_Others'] = list()    \n",
    "    sorted_retrieved['PER_Victim'] = list()    \n",
    "    sorted_retrieved['PER_Accused'] = list()    \n",
    "    sorted_retrieved['ORG_Victim'] = list()    \n",
    "    sorted_retrieved['ORG_Accused'] = list()    \n",
    "    sorted_retrieved['ORG_Others'] = list()    \n",
    "    sorted_retrieved['LOC_Accused'] = list()    \n",
    "    sorted_retrieved['LOC_Others'] = list()    \n",
    "    sorted_retrieved['LOC_Event'] = list()\n",
    "    sorted_retrieved['LOC_Victim'] = list()    \n",
    "    sorted_retrieved['PER_Assoc'] = list()    \n",
    "    sorted_retrieved['ORG_Assoc'] = list()    \n",
    "    sorted_retrieved['LOC_Assoc'] = list()\n",
    "    sorted_retrieved['Event'] = list()\n",
    "    sorted_retrieved['O'] = list()\n",
    "    sorted_retrieved['Victim'] = list()\n",
    "    sorted_retrieved['Location'] = list()\n",
    "    sorted_retrieved['Accused'] = list()\n",
    "    \n",
    "    retrieved_count['PER_Others'] = dict()    \n",
    "    retrieved_count['PER_Victim'] = dict()    \n",
    "    retrieved_count['PER_Accused'] = dict()    \n",
    "    retrieved_count['ORG_Victim'] = dict()    \n",
    "    retrieved_count['ORG_Accused'] = dict()    \n",
    "    retrieved_count['ORG_Others'] = dict()    \n",
    "    retrieved_count['LOC_Accused'] = dict()    \n",
    "    retrieved_count['LOC_Others'] = dict()    \n",
    "    retrieved_count['LOC_Event'] = dict()\n",
    "    retrieved_count['LOC_Victim'] = dict()    \n",
    "    retrieved_count['PER_Assoc'] = dict()    \n",
    "    retrieved_count['ORG_Assoc'] = dict()    \n",
    "    retrieved_count['LOC_Assoc'] = dict()\n",
    "    retrieved_count['Event'] = dict()\n",
    "    retrieved_count['O'] = dict()\n",
    "    retrieved_count['Victim'] = dict()\n",
    "    retrieved_count['Location'] = dict()\n",
    "    retrieved_count['Accused'] = dict()\n",
    "\n",
    "    \n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            \n",
    "            # This is number of all words which is N \n",
    "            length = min(len(word_in_file), len(tag_in_file))                \n",
    "            for j in xrange(length):\n",
    "                # Get the word embedding of the word\n",
    "                word_embed = np.zeros(word_dim)\n",
    "                \n",
    "                if (j>(win_size-1)) and (j<length-win_size):\n",
    "                    replace_unk = 'unk'\n",
    "                    for k in xrange(2*win_size + 1):\n",
    "                        if word_in_file[j-win_size+k].lower() in glove_model:\n",
    "                            word_embed = np.concatenate((word_embed, glove_model[word_in_file[j-win_size+k].lower()]), axis = 0)\n",
    "                        else:\n",
    "                            word_embed = np.concatenate((word_embed, glove_model['unk']), axis=0)  \n",
    "                    \n",
    "                    for tag in tagEmbedding:\n",
    "                        dist = 1 - (dot(tagEmbedding[tag], word_embed)/(norm(tagEmbedding[tag]) * norm(word_embed)))\n",
    "                        if word_in_file[j] in retrieved[tag].keys():\n",
    "                            retrieved[tag][word_in_file[j]] = retrieved[tag][word_in_file[j]]+dist\n",
    "                            retrieved_count[tag][word_in_file[j]] = retrieved_count[tag][word_in_file[j]] + 1\n",
    "                        else:\n",
    "                            retrieved[tag][word_in_file[j]] = dist\n",
    "                            retrieved_count[tag][word_in_file[j]] = 1\n",
    "                        \n",
    "                    relevant[tag_in_file[j]] =  word_in_file[j]\n",
    "   \n",
    "    #take the average of score of tags (TotalScore/Occurences)\n",
    "    for tag in tagEmbedding:\n",
    "        for key in retrieved[tag]:\n",
    "            retrieved[tag][key] = retrieved[tag][key]/retrieved_count[tag][key]  \n",
    "    \n",
    "    # Sort the retrieved results by score\n",
    "    for tag in tagEmbedding:\n",
    "        sorted_retrieved[tag] = sorted(retrieved[tag].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #Calculate Precison and Recall for this file\n",
    "    try:\n",
    "        for tag in tagEmbedding:\n",
    "            tp = 0\n",
    "            if len(relevant[tag]) == 0:\n",
    "                num_docs_dont_have_tag[tag] = num_docs_dont_have_tag[tag] +1\n",
    "            else:\n",
    "                for i in xrange(topN):\n",
    "                    if sorted_retrieved[tag][i][0] in relevant[tag]:\n",
    "                        tp = tp+1\n",
    "                Precision[tag] = Precision[tag] + (float(tp*100)/topN)\n",
    "                Recall[tag] = Recall[tag]+ (float(tp*100)/len(relevant[tag]))\n",
    "                \n",
    "    except IndexError:\n",
    "        print f\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER_Assoc 1.33333333333% 0.833333333333% 1.02564102564\n",
      "ORG_Assoc\n",
      "PER_Others 0.110497237569% 0.110497237569% 0.110497237569\n",
      "Victim\n",
      "LOC_Event 1.3% 0.886849700008% 1.0543976662\n",
      "Accused 1.33333333333% 0.952380952381% 1.11111111111\n",
      "O 0.338983050847% 0.548157115954% 0.418910062837\n",
      "ORG_Accused 0.582524271845% 0.380258899676% 0.460145223138\n",
      "PER_Victim 1.05263157895% 0.921052631579% 0.982456140351\n",
      "ORG_Victim 1.25% 0.892857142857% 1.04166666667\n",
      "LOC_Others 0.820512820513% 0.781237281237% 0.800393525079\n",
      "ORG_Others 0.398009950249% 0.394456289979% 0.396225152266\n",
      "PER_Accused 1.31578947368% 1.07038429407% 1.18046758033\n",
      "LOC_Accused 1.11111111111% 0.705467372134% 0.862998921251\n",
      "LOC_Assoc 2.30769230769% 2.69230769231% 2.48520710059\n",
      "LOC_Victim 3.63636363636% 3.57142857143% 3.6036036036\n",
      "Event\n",
      "Location 0.625% 0.520833333333% 0.568181818182\n"
     ]
    }
   ],
   "source": [
    "num_test_file = len(test_file_list)\n",
    "for tag in tagEmbedding:\n",
    "    try:\n",
    "        precision = float(Precision[tag])/(num_test_file-num_docs_dont_have_tag[tag])\n",
    "        recall = float(Recall[tag])/(num_test_file-num_docs_dont_have_tag[tag])\n",
    "        f1 = float(2*precision*recall)/(precision+recall)\n",
    "        print tag, str(precision)+'%', str(recall)+'%', f1\n",
    "    except:\n",
    "        print tag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
