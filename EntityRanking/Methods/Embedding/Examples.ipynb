{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent.lower())\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print word_tokens\n",
    "print filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n",
      "[(0, 0), (1, 2), (2, 1), (3, 4), (4, 3)]\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Sort the dictionay by value\n",
    "import operator\n",
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "print sorted_x\n",
    "\n",
    "\n",
    "# Sort the dictionay by key\n",
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(0))\n",
    "print sorted_x\n",
    "\n",
    "# Sort the dictionay by value in descending\n",
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'said', 'that', 's', 'it', 'Hello', 'World']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuations\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = '''He said,\"that's it.\" *u* Hello, World.'''\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "toker.tokenize(s)\n",
    "['He', 'said', 'that', 's', 'it', 'Hello', 'World'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Glove Embedding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gloveFile = \"glove.6B/glove.6B.50d.txt\"\n",
    "\n",
    "model = {}\n",
    "\n",
    "with open(gloveFile, 'r') as f:\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -7.91490000e-01   8.66170000e-01   1.19980000e-01   9.22870000e-04\n",
      "   2.77600000e-01  -4.91850000e-01   5.01950000e-01   6.07920000e-04\n",
      "  -2.58450000e-01   1.78650000e-01   2.53500000e-01   7.65720000e-01\n",
      "   5.06640000e-01   4.02500000e-01  -2.13880000e-03  -2.83970000e-01\n",
      "  -5.03240000e-01   3.04490000e-01   5.17790000e-01   1.50900000e-02\n",
      "  -3.50310000e-01  -1.12780000e+00   3.32530000e-01  -3.52500000e-01\n",
      "   4.13260000e-02   1.08630000e+00   3.39100000e-02   3.35640000e-01\n",
      "   4.97450000e-01  -7.01310000e-02  -1.21920000e+00  -4.85120000e-01\n",
      "  -3.85120000e-02  -1.35540000e-01  -1.63800000e-01   5.23210000e-01\n",
      "  -3.13180000e-01  -1.65500000e-01   1.19090000e-01  -1.51150000e-01\n",
      "  -1.56210000e-01  -6.26550000e-01  -6.23360000e-01  -4.21500000e-01\n",
      "   4.18730000e-01  -9.24720000e-01   1.10490000e+00  -2.99960000e-01\n",
      "  -6.30030000e-03   3.95400000e-01]\n",
      "[ -7.91490000e-01   8.66170000e-01   1.19980000e-01   9.22870000e-04\n",
      "   2.77600000e-01  -4.91850000e-01   5.01950000e-01   6.07920000e-04\n",
      "  -2.58450000e-01   1.78650000e-01   2.53500000e-01   7.65720000e-01\n",
      "   5.06640000e-01   4.02500000e-01  -2.13880000e-03  -2.83970000e-01\n",
      "  -5.03240000e-01   3.04490000e-01   5.17790000e-01   1.50900000e-02\n",
      "  -3.50310000e-01  -1.12780000e+00   3.32530000e-01  -3.52500000e-01\n",
      "   4.13260000e-02   1.08630000e+00   3.39100000e-02   3.35640000e-01\n",
      "   4.97450000e-01  -7.01310000e-02  -1.21920000e+00  -4.85120000e-01\n",
      "  -3.85120000e-02  -1.35540000e-01  -1.63800000e-01   5.23210000e-01\n",
      "  -3.13180000e-01  -1.65500000e-01   1.19090000e-01  -1.51150000e-01\n",
      "  -1.56210000e-01  -6.26550000e-01  -6.23360000e-01  -4.21500000e-01\n",
      "   4.18730000e-01  -9.24720000e-01   1.10490000e+00  -2.99960000e-01\n",
      "  -6.30030000e-03   3.95400000e-01]\n"
     ]
    }
   ],
   "source": [
    "# Less frequent words are labeled as <unk>\n",
    "print model['unk']\n",
    "\n",
    "key = 'New Delhi'\n",
    "\n",
    "if key in model:\n",
    "    print model[key]\n",
    "else:\n",
    "    print model['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Check if key is in dict\n",
    "d = dict()\n",
    "d[\"neelesh\"] = 1\n",
    "\n",
    "key = \"neelesh\"\n",
    "\n",
    "if key in d:\n",
    "    d[key] += 1\n",
    "else:\n",
    "    d[key] = 1\n",
    "\n",
    "print d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   2.   4.]\n",
      " [  3.   5.   7.]\n",
      " [  6.   8.  10.]]\n"
     ]
    }
   ],
   "source": [
    "# add two array\n",
    "word_embed = np.array\n",
    "np.add(1.0, 4.0)\n",
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "\n",
    "x2 = np.arange(3.0)\n",
    "x1 = np.add(x1, x2)\n",
    "print x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n"
     ]
    }
   ],
   "source": [
    "sent = \"This IS MY LIFE\"\n",
    "words = sent.split()\n",
    "print words[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cosine_similarity = dot([-1, 0, -1], [1,0, 1])/(norm([-1, 0, -1]) * norm([1,0, 1]))\n",
    "\n",
    "print cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate two arrays\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([5, 6])\n",
    "np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER\n"
     ]
    }
   ],
   "source": [
    "tag = 'PER_Others'\n",
    "print tag[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
