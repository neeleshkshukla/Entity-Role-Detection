{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Sequence Generated\n"
     ]
    }
   ],
   "source": [
    "# Create tag sequence\n",
    "\n",
    "tag_dir = 'Data/new_tags'\n",
    "sequencePath = 'Data/tag_sequence.txt'\n",
    "\n",
    "fout = open(sequencePath, 'w')\n",
    "\n",
    "for f in os.listdir(tag_dir):\n",
    "    filepath = os.path.join(tag_dir, f)\n",
    "    with open(filepath, \"rt\") as fin:\n",
    "        for line in fin:\n",
    "            fout.write(line)\n",
    "    fout.write(\" \")\n",
    "\n",
    "fout.close()\n",
    "\n",
    "print 'Tag Sequence Generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "O O O O O O O O O O O O O O O O ORG_Others O O O O O O O PER_Others PER_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O LOC_Event O O O O O\n",
      "('word counts: ', 19, 'text len: ', 487611)\n",
      "Vocabulary Length = 21\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "\n",
    "with open(sequencePath, 'r') as sequenceFile:\n",
    "        s_text = sequenceFile.read().replace('\\n', '')\n",
    "print('Sample data:\\n'+ s_text[:200])\n",
    "\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    print ('word counts: ', len(word_counts), 'text len: ', len(text.split(' ')))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key: val for key, val in word_counts.items() if val > min_word_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key: (ix + 1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown'] = 0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val: key for key, val in vocab_to_ix_dict.items()}\n",
    "    return (ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "\n",
    "# Build Sequence vocabulary\n",
    "min_word_freq = 0  # Trim the less frequent words off\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "# Sanity Check\n",
    "assert (len(ix2vocab) == len(vocab2ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to word \n",
    "\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "class LSTM_Model():\n",
    "    def __init__(self, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "\n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "\n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "\n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            print('xdata:', self.x_data.get_shape())\n",
    "            print('emb_mat: ', embedding_mat.get_shape())\n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            print('emb_output: ', embedding_output.get_shape())\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            print('rnninputs: ', len(rnn_inputs), rnn_inputs[0].get_shape())\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "            print('rnninput trimmed:', len(rnn_inputs_trimmed), rnn_inputs_trimmed[0].get_shape())\n",
    "\n",
    "            # If we are inferring (generating text), we add a 'loop' function\n",
    "            # Define how to get the i+1 th input from the i th output\n",
    "            def inferred_loop(prev, count):\n",
    "                # Apply hidden layer\n",
    "                prev_transformed = tf.matmul(prev, W) + b\n",
    "                # Get the index of the output (also don't run the gradient)\n",
    "                prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "                # Get embedded vector\n",
    "                output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "                return (output)\n",
    "\n",
    "            decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "            outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                          self.initial_state,\n",
    "                                          self.lstm_cell,\n",
    "                                          loop_function=inferred_loop if infer_sample else None)\n",
    "            # Non inferred outputs\n",
    "            output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "            # Logits and output\n",
    "            self.logit_output = tf.matmul(output, W) + b\n",
    "            self.model_output = tf.nn.softmax(self.logit_output)\n",
    "\n",
    "            loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "            loss = loss_fun([self.logit_output], [tf.reshape(self.y_output, [-1])],\n",
    "                            [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                            self.vocab_size)\n",
    "            self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "            self.final_state = last_state\n",
    "            gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "\n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=1, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return (out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('xdata:', TensorShape([Dimension(32), Dimension(3)]))\n",
      "('emb_mat: ', TensorShape([Dimension(21), Dimension(1024)]))\n",
      "('emb_output: ', TensorShape([Dimension(32), Dimension(3), Dimension(1024)]))\n",
      "('rnninputs: ', 3, TensorShape([Dimension(32), Dimension(1), Dimension(1024)]))\n",
      "('rnninput trimmed:', 3, TensorShape([Dimension(32), Dimension(1024)]))\n",
      "('xdata:', TensorShape([Dimension(1), Dimension(1)]))\n",
      "('emb_mat: ', TensorShape([Dimension(21), Dimension(1024)]))\n",
      "('emb_output: ', TensorShape([Dimension(1), Dimension(1), Dimension(1024)]))\n",
      "('rnninputs: ', 1, TensorShape([Dimension(1), Dimension(1), Dimension(1024)]))\n",
      "('rnninput trimmed:', 1, TensorShape([Dimension(1), Dimension(1024)]))\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 1024  # RNN Model size, has to equal embedding size\n",
    "epochs = 10  # Number of epochs to cycle through data\n",
    "batch_size = 32  # Train on this many examples at once\n",
    "learning_rate = 0.001  # Learning rate\n",
    "training_seq_len = 3  # how long of a word group to consider #Work with Trigram\n",
    "embedding_size = rnn_size\n",
    "eval_every = 50  # How often to evaluate the test sentences\n",
    "prime_texts = ['11110 3564 2460', '9394 5392 5862 12667', '10007 5735 589']\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define LSTM RNN Model\n",
    "with tf.variable_scope('lstm_model') as scope:\n",
    "    # Define LSTM Model\n",
    "    lstm_model = LSTM_Model(rnn_size, batch_size, learning_rate,\n",
    "                            training_seq_len, vocab_size)\n",
    "    scope.reuse_variables()\n",
    "    test_lstm_model = LSTM_Model(rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(s_text_ix) / (batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 5081, Loss: 10.68\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 5081, Loss: 1.94\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 5081, Loss: 3.15\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 5081, Loss: 1.42\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 5081, Loss: 2.34\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'11110'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6fff76059904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_lstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix2vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab2ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0miteration_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-40f7714e6d8d>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sess, words, vocab, num, prime_text)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '11110'"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch + 1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "\n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "\n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch + 1, ix + 1, num_batches + 1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "\n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "\n",
    "        iteration_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
