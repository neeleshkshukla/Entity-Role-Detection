{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import CRFTagger\n",
    "import os\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for NLTK HMM Tagger\n",
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "doc_dir = '../../../../Data/input/rectified/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print (len(train_file_list))\n",
    "print (len(test_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Tagged dataset to be fed in nltk hmm tagger\n",
    "# It is a list of tagged sentences. \n",
    "# training_data = [ [(word, tag), (word, tag).....]\n",
    "# [(word, tag), (word, tag).....]\n",
    "#]\n",
    "\n",
    "# Directory having tags\n",
    "tag_dir = '../../../../Data/input/rectified/new_tags'\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for f in train_file_list:\n",
    "    training_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with io.open(word_file_path, \"rt\", encoding=\"utf-8\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with io.open(tag_file_path, \"rt\", encoding=\"utf-8\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in range(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in range(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            training_sentences.append(pairs_in_line)\n",
    "    if len(training_sentences) > 0:\n",
    "        training_data.extend(training_sentences)\n",
    "\n",
    "#print training_data[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "ct.train(training_data,'model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('death', 'O'), ('toll', 'O'), ('from', 'O'), ('a', 'O'), ('suicide', 'O'), ('bombing', 'O'), ('at', 'O'), ('a', 'O'), ('coffee', 'O'), ('house', 'O'), ('in', 'O'), ('central', 'LOC_Others'), ('Iraq', 'LOC_Others'), (\"'s\", 'O'), ('restive', 'O'), ('Diyala', 'LOC_Event'), ('province', 'LOC_Event'), ('has', 'O'), ('risen', 'O'), ('to', 'O'), ('30', 'O'), (',', 'O'), ('a', 'O'), ('medical', 'O'), ('official', 'O'), ('said', 'O'), ('on', 'O'), ('Saturday', 'O'), (',', 'O'), ('making', 'O'), ('the', 'O'), ('deadliest', 'O'), ('attack', 'O'), ('in', 'O'), ('October', 'O'), ('.', 'O'), ('Friday', 'O'), ('night', 'O'), (\"'s\", 'O'), ('bombing', 'O'), ('in', 'O'), ('the', 'O'), ('town', 'O'), ('of', 'O'), ('Balad', 'PER_Accused'), ('Ruz', 'PER_Accused'), ('also', 'O'), ('wounded', 'O'), ('68', 'O'), ('people', 'O'), (',', 'O'), ('according', 'O'), ('to', 'O'), ('Ahmed', 'PER_Accused'), ('Alwan', 'PER_Accused'), (',', 'O'), ('a', 'O'), ('doctor', 'O'), ('at', 'O'), ('the', 'O'), ('general', 'O'), ('hospital', 'O'), ('of', 'O'), ('Baquba', 'LOC_Event'), (',', 'O'), ('the', 'O'), ('provincial', 'O'), ('capital', 'O'), ('of', 'O'), ('Diyala', 'LOC_Event'), ('.', 'O'), ('The', 'O'), ('final', 'O'), ('toll', 'O'), ('from', 'O'), ('Friday', 'O'), (\"'s\", 'O'), ('bombing', 'O'), ('is', 'O'), ('30', 'O'), ('dead', 'O'), ('and', 'O'), ('68', 'O'), ('wounded', 'O'), ('.', 'O'), ('All', 'O'), ('the', 'O'), ('dead', 'O'), ('are', 'O'), ('men', 'O'), (',', 'O'), ('but', 'O'), ('the', 'O'), ('wounded', 'O'), ('include', 'O'), ('three', 'O'), ('women', 'O'), ('and', 'O'), ('two', 'O'), ('children', 'O'), (',', 'O'), ('Dr.', 'O'), ('Alwan', 'O'), ('said', 'O'), ('.', 'O'), ('Balad', 'ORG_Others'), ('Ruz', 'ORG_Others'), ('police', 'ORG_Others'), ('chief', 'O'), ('Major', 'O'), ('Ahmed', 'O'), ('al-Tamimi', 'O'), ('had', 'O'), ('given', 'O'), ('on', 'O'), ('Friday', 'O'), ('a', 'O'), ('toll', 'O'), ('of', 'O'), ('25', 'O'), ('dead', 'O'), ('and', 'O'), ('70', 'O'), ('wounded', 'O'), ('when', 'O'), ('a', 'O'), ('suicide', 'O'), ('bomber', 'O'), ('detonated', 'O'), ('a', 'O'), ('belt', 'O'), ('filled', 'O'), ('with', 'O'), ('explosives', 'O'), ('at', 'O'), ('a', 'O'), ('coffee', 'O'), ('house', 'O'), ('in', 'O'), ('the', 'O'), ('centre', 'O'), ('of', 'O'), ('the', 'O'), ('city', 'O'), ('.', 'O'), ('Fall', 'O'), ('in', 'O'), ('bombings', 'O'), ('Suicide', 'O'), ('bombings', 'O'), ('across', 'O'), ('Iraq', 'LOC_Others'), ('have', 'O'), ('fallen', 'O'), ('since', 'O'), ('2008', 'O'), ('when', 'O'), ('Diyala', 'LOC_Event'), ('province', 'LOC_Event'), ('was', 'O'), ('an', 'O'), ('Al-Qaeda', 'ORG_Accused'), ('stronghold', 'O'), ('.', 'O'), ('Although', 'O'), ('the', 'O'), ('militants', 'O'), ('have', 'O'), ('lost', 'O'), ('ground', 'O'), (',', 'O'), ('the', 'O'), ('province', 'O'), ('remains', 'O'), ('one', 'O'), ('of', 'O'), ('the', 'O'), ('most', 'O'), ('restive', 'O'), ('since', 'O'), ('the', 'O'), ('2003', 'O'), ('U.S.-led', 'O'), ('invasion', 'O'), ('.', 'O'), ('Friday', 'O'), (\"'s\", 'O'), ('attack', 'O'), ('took', 'O'), ('place', 'O'), ('in', 'O'), ('a', 'O'), ('region', 'O'), ('with', 'O'), ('a', 'O'), ('Kurdish', 'PER_Others'), ('Shia', 'PER_Others'), ('majority', 'O'), ('.', 'O'), ('Baquba', 'LOC_Event'), (',', 'O'), ('once', 'O'), ('a', 'O'), ('stronghold', 'O'), ('of', 'O'), ('Al-Qaeda', 'ORG_Accused'), (',', 'O'), ('is', 'O'), ('the', 'O'), ('ethnically', 'O'), ('mixed', 'O'), ('capital', 'O'), ('of', 'O'), ('Diyala', 'LOC_Event'), (',', 'O'), ('which', 'O'), ('remains', 'O'), ('one', 'O'), ('of', 'O'), ('the', 'O'), ('country', 'O'), (\"'s\", 'O'), ('most', 'O'), ('unstable', 'O'), ('provinces', 'O'), ('.', 'O'), ('The', 'O'), ('blast', 'O'), ('was', 'O'), ('the', 'O'), ('first', 'O'), ('major', 'O'), ('attack', 'O'), ('in', 'O'), ('Iraq', 'LOC_Others'), ('in', 'O'), ('several', 'O'), ('weeks', 'O'), (',', 'O'), ('as', 'O'), ('a', 'O'), ('seven-month', 'O'), ('political', 'O'), ('stalemate', 'O'), ('surrounding', 'O'), ('the', 'O'), ('formation', 'O'), ('of', 'O'), ('a', 'O'), ('new', 'O'), ('government', 'O'), ('has', 'O'), ('dragged', 'O'), ('on', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "for f in test_file_list:\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    with io.open(word_file_path, \"rt\", encoding=\"utf-8\") as word_file:\n",
    "        for line in word_file:\n",
    "            predicted_tags = ct.tag_sents([line.split()])\n",
    "            print (predicted_tags[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ORG_Victim': 111, 'LOC_Event': 1402, 'PER_Accused': 577, 'LOC_Victim': 36, 'ORG_Accused': 637, 'PER_Victim': 271, 'LOC_Accused': 152, 'PER_Others': 1579, 'LOC_Others': 1486, 'ORG_Others': 1961}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "relevant_tag_count = dict() # TP + FN\n",
    "relevant_retrieved_tag_count = dict() # TP\n",
    "retrieved_tag_count = dict() # TP + FP\n",
    "\n",
    "relevant_tag_count[u'PER_Others'] = 0\n",
    "relevant_retrieved_tag_count[u'PER_Others'] = 0\n",
    "retrieved_tag_count[u'PER_Others'] = 0\n",
    "\n",
    "relevant_tag_count[u'PER_Victim'] = 0\n",
    "relevant_retrieved_tag_count[u'PER_Victim'] = 0\n",
    "retrieved_tag_count[u'PER_Victim'] = 0\n",
    "\n",
    "relevant_tag_count[u'PER_Accused'] = 0\n",
    "relevant_retrieved_tag_count[u'PER_Accused'] = 0\n",
    "retrieved_tag_count[u'PER_Accused'] = 0\n",
    "\n",
    "relevant_tag_count[u'ORG_Victim'] = 0\n",
    "relevant_retrieved_tag_count[u'ORG_Victim'] = 0\n",
    "retrieved_tag_count[u'ORG_Victim'] = 0\n",
    "\n",
    "relevant_tag_count[u'ORG_Accused'] = 0\n",
    "relevant_retrieved_tag_count[u'ORG_Accused'] = 0\n",
    "retrieved_tag_count[u'ORG_Accused'] = 0\n",
    "\n",
    "relevant_tag_count[u'ORG_Others'] = 0\n",
    "relevant_retrieved_tag_count[u'ORG_Others'] = 0\n",
    "retrieved_tag_count[u'ORG_Others'] = 0\n",
    "\n",
    "relevant_tag_count[u'LOC_Accused'] = 0\n",
    "relevant_retrieved_tag_count[u'LOC_Accused'] = 0\n",
    "retrieved_tag_count[u'LOC_Accused'] = 0\n",
    "\n",
    "relevant_tag_count[u'LOC_Others'] = 0\n",
    "relevant_retrieved_tag_count[u'LOC_Others'] = 0\n",
    "retrieved_tag_count[u'LOC_Others'] = 0\n",
    "\n",
    "relevant_tag_count[u'LOC_Event'] = 0\n",
    "relevant_retrieved_tag_count[u'LOC_Event'] = 0\n",
    "retrieved_tag_count[u'LOC_Event'] = 0\n",
    "\n",
    "relevant_tag_count[u'LOC_Victim'] = 0\n",
    "relevant_retrieved_tag_count[u'LOC_Victim'] = 0\n",
    "retrieved_tag_count[u'LOC_Victim'] = 0\n",
    "\n",
    "for f in test_file_list:\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    predicted_tags = []\n",
    "    with io.open(word_file_path, \"rt\", encoding=\"utf-8\") as word_file:\n",
    "        for line in word_file:\n",
    "            predicted_tags = ct.tag_sents([line.split()])\n",
    "            #print predicted_tags\n",
    "    if len(predicted_tags) > 0 and len(predicted_tags[0]) > 0:\n",
    "        actual_tags = []\n",
    "        with io.open(tag_file_path, \"rt\", encoding=\"utf-8\") as tag_file:\n",
    "            for line in tag_file:\n",
    "                #print(line + '\\n')\n",
    "                actual_tags = line.split()\n",
    "                #print actual_tags\n",
    "        result_len = min(len(predicted_tags[0]), len(actual_tags))\n",
    "        \n",
    "        for i in range(result_len):\n",
    "            #print predicted_tags[i][1], actual_tags[i]\n",
    "            if actual_tags[i] in relevant_tag_count.keys():\n",
    "                relevant_tag_count[actual_tags[i]] = relevant_tag_count[actual_tags[i]] + 1\n",
    "                \n",
    "            if predicted_tags[0][i][1] in retrieved_tag_count.keys():\n",
    "                retrieved_tag_count[predicted_tags[0][i][1]] = retrieved_tag_count[predicted_tags[0][i][1]] + 1\n",
    "                \n",
    "            if actual_tags[i] == predicted_tags[0][i][1]:\n",
    "                if actual_tags[i] in relevant_retrieved_tag_count.keys():\n",
    "                    relevant_retrieved_tag_count[actual_tags[i]] = relevant_retrieved_tag_count[actual_tags[i]] + 1\n",
    "\n",
    "print(relevant_tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Precision Class Wise =====================\n",
      "\n",
      "Class    Precision %\n",
      "--------------------------\n",
      "ORG_Victim 20.0\n",
      "LOC_Event 57.18725718725719\n",
      "PER_Accused 53.546910755148744\n",
      "LOC_Victim 0.0\n",
      "ORG_Accused 83.87650085763293\n",
      "PER_Victim 46.478873239436616\n",
      "LOC_Accused 25.0\n",
      "\n",
      "======= Avergae Precision =====================\n",
      "\n",
      "Average Precision:  40.86993457706792\n"
     ]
    }
   ],
   "source": [
    "avg_precision = 0\n",
    "\n",
    "print('======= Precision Class Wise =====================\\n')\n",
    "\n",
    "print('Class    Precision %')\n",
    "print('--------------------------')\n",
    "\n",
    "count_keys = 0\n",
    "for key in retrieved_tag_count.keys():\n",
    "    if key[4:]!='Others':\n",
    "        count_keys = count_keys + 1\n",
    "        if retrieved_tag_count[key] != 0:\n",
    "            prec = float(relevant_retrieved_tag_count[key]*100)/retrieved_tag_count[key]\n",
    "            print(key, prec)\n",
    "            avg_precision = avg_precision + prec\n",
    "        else:\n",
    "            print(key, 0.0)\n",
    "        \n",
    "print('\\n======= Avergae Precision =====================\\n')\n",
    "print('Average Precision: ',avg_precision/count_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
