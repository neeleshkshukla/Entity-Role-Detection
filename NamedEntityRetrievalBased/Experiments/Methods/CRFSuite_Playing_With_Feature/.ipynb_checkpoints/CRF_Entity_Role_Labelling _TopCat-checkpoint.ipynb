{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "# To test for PER, LOC and ORG Only\n",
    "\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "\n",
    "doc_dir = 'Data/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print len(train_file_list)\n",
    "print len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Tagged dataset to be fed in nltk hmm tagger\n",
    "# It is a list of tagged sentences. \n",
    "# training_data = [ [(word, tag), (word, tag).....]\n",
    "# [(word, tag), (word, tag).....]\n",
    "#]\n",
    "\n",
    "# Directory having tags\n",
    "tag_dir = 'Data/new_tags'\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for f in train_file_list:\n",
    "    training_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in xrange(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            training_sentences.append(pairs_in_line)\n",
    "    if len(training_sentences) > 0:\n",
    "        training_data.extend(training_sentences)\n",
    "\n",
    "#print training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        tag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "           # '-1:tag1=' + tag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2tags(sent):\n",
    "    return [tag for word, tag in sent]\n",
    "\n",
    "def sent2words(sent):\n",
    "    return [word for word, tag in sent] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Five', 'O'), ('people', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('over', 'O'), ('50', 'O'), ('injured', 'O'), ('in', 'O'), ('three', 'O'), ('blasts', 'O'), ('set', 'O'), ('off', 'O'), ('by', 'O'), ('insurgent', 'O'), ('outfit', 'O'), ('ULFA', 'ORG_Others'), ('here', 'O'), ('today', 'O'), ('hours', 'O'), ('before', 'O'), ('Union', 'O'), ('Home', 'O'), ('Minister', 'O'), ('P', 'PER_Others'), ('Chidambaram', 'PER_Others'), (\"'s\", 'O'), ('visit', 'O'), ('to', 'O'), ('review', 'O'), ('law', 'O'), ('and', 'O'), ('order', 'O'), ('situation', 'O'), ('in', 'O'), ('the', 'O'), ('state', 'O'), ('rocked', 'O'), ('by', 'O'), ('deadly', 'O'), ('blasts', 'O'), ('that', 'O'), ('left', 'O'), ('88', 'O'), ('dead', 'O'), ('two', 'O'), ('months', 'O'), ('ago', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Three', 'O'), ('people', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('35', 'O'), ('injured', 'O'), (',', 'O'), ('including', 'O'), ('four', 'O'), ('women', 'O'), (',', 'O'), ('when', 'O'), ('a', 'O'), ('bomb', 'O'), ('planted', 'O'), ('in', 'O'), ('front', 'O'), ('of', 'O'), ('a', 'O'), ('closed', 'O'), ('sweet', 'O'), ('shop', 'O'), ('exploded', 'O'), ('near', 'O'), ('upmarket', 'O'), ('Bhangagarh', 'LOC_Event'), ('flyover', 'O'), ('on', 'O'), ('the', 'O'), ('busy', 'O'), ('Guwahati-Shillong', 'O'), ('road', 'O'), ('at', 'O'), ('around', 'O'), ('5:45', 'O'), ('pm', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Two', 'O'), ('persons', 'O'), ('died', 'O'), ('on', 'O'), ('way', 'O'), ('to', 'O'), ('hospital', 'O'), ('while', 'O'), ('another', 'O'), ('succumbed', 'O'), ('to', 'O'), ('his', 'O'), ('injuries', 'O'), ('at', 'O'), ('the', 'O'), ('Guwahati', 'ORG_Others'), ('Medical', 'ORG_Others'), ('College', 'ORG_Others'), ('Hospital', 'ORG_Others'), (',', 'O'), ('DGP', 'O'), ('G', 'O'), ('M', 'O'), ('Srivastava', 'PER_Others'), ('said', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Two', 'O'), ('of', 'O'), ('the', 'O'), ('dead', 'O'), ('were', 'O'), ('identified', 'O'), ('as', 'O'), ('Amal', 'PER_Victim'), ('Das', 'PER_Victim'), ('and', 'O'), ('Kahil', 'PER_Victim'), ('Sheikh', 'PER_Victim'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('A', 'O'), ('bomb', 'O'), ('kept', 'O'), ('in', 'O'), ('a', 'O'), ('bicycle', 'O'), ('exploded', 'O'), ('at', 'O'), ('a', 'O'), ('market', 'O'), ('in', 'O'), ('the', 'O'), ('Bhootnath', 'LOC_Event'), ('area', 'O'), (',', 'O'), ('the', 'O'), ('route', 'O'), ('which', 'O'), ('Chidambaram', 'PER_Others'), ('was', 'O'), ('to', 'O'), ('take', 'O'), ('on', 'O'), ('his', 'O'), ('way', 'O'), ('from', 'O'), ('the', 'O'), ('airport', 'O'), (',', 'O'), ('under', 'O'), ('Baralumukh', 'LOC_Others'), ('police', 'O'), ('station', 'O'), ('around', 'O'), ('5.30', 'O'), ('pm', 'O'), ('in', 'O'), ('which', 'O'), ('two', 'O'), ('persons', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('12', 'O'), ('others', 'O'), ('injured', 'O'), (',', 'O'), ('official', 'O'), ('sources', 'O'), ('said', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('An', 'O'), ('Improvised', 'O'), ('Explosive', 'O'), ('Devise', 'O'), ('(', 'O'), ('IED', 'O'), (')', 'O'), ('kept', 'O'), ('in', 'O'), ('a', 'O'), ('Gauhati', 'O'), ('Municipal', 'O'), ('Corporation', 'O'), ('(', 'O'), ('GMC', 'O'), (')', 'O'), ('dustbin', 'O'), ('went', 'O'), ('off', 'O'), ('at', 'O'), ('around', 'O'), ('3.30', 'O'), ('pm', 'O'), ('injuring', 'O'), ('three', 'O'), ('persons', 'O'), ('in', 'O'), ('Birubari', 'LOC_Event'), ('Tiniali', 'LOC_Event'), ('area', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('The', 'O'), ('blasts', 'O'), ('came', 'O'), ('a', 'O'), ('day', 'O'), ('ahead', 'O'), ('of', 'O'), ('Prime', 'O'), ('Minister', 'O'), ('Manmohan', 'PER_Others'), ('Singh', 'PER_Others'), (\"'s\", 'O'), ('arrival', 'O'), ('here', 'O'), ('tomorrow', 'O'), ('enroute', 'O'), ('Shillong', 'LOC_Others'), ('to', 'O'), ('inaugurate', 'O'), ('the', 'O'), ('Indian', 'ORG_Others'), ('Science', 'ORG_Others'), ('Congress', 'ORG_Others'), ('there', 'O'), ('on', 'O'), ('January', 'O'), ('three', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Chidmabaram', 'PER_Others'), ('arrived', 'O'), ('at', 'O'), ('the', 'O'), ('Lokopriyo', 'LOC_Others'), ('Gopinath', 'LOC_Others'), ('Bordoloi', 'LOC_Others'), ('international', 'LOC_Others'), ('airport', 'LOC_Others'), ('on', 'O'), ('a', 'O'), ('two-day', 'O'), ('visit', 'O'), ('to', 'O'), ('the', 'O'), ('state', 'O'), ('to', 'O'), ('review', 'O'), ('Assam', 'LOC_Event'), (\"'s\", 'O'), ('law', 'O'), ('and', 'O'), ('order', 'O'), ('situation', 'O'), ('and', 'O'), ('attend', 'O'), ('a', 'O'), ('meeting', 'O'), ('of', 'O'), ('the', 'O'), ('Unified', 'O'), ('Command', 'O'), ('headed', 'O'), ('by', 'O'), ('Chief', 'O'), ('Minister', 'O'), ('Tarun', 'PER_Others'), ('Gogoi', 'PER_Others'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O')]\n",
      "['bias', 'word.lower=five', 'word[-3:]=ive', 'word[-2:]=ve', 'word.isupper=False', 'word.istitle=True', 'word.isdigit=False', 'BOS', '+1:word.lower=people', '+1:word.istitle=False', '+1:word.isupper=False']\n",
      "O\n",
      "Five\n"
     ]
    }
   ],
   "source": [
    "print training_data[0]\n",
    "print sent2features(training_data[0])[0]\n",
    "print sent2tags(training_data[0])[0]\n",
    "print sent2words(training_data[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sent2features(s) for s in training_data]\n",
    "y_train = [sent2tags(s) for s in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the model\n",
    "trainer.train('osint-bomb-blast.crfsuite.topcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f658afd61d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('osint-bomb-blast.crfsuite.topcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ammonium', 'O'), ('nitrate', 'O'), ('was', 'O'), ('used', 'O'), ('to', 'O'), ('make', 'O'), ('the', 'O'), ('bombs', 'O'), ('used', 'O'), ('in', 'O'), ('the', 'O'), ('Lucknow', 'LOC_Others'), ('and', 'O'), ('Faizabad', 'LOC_Others'), ('blasts', 'O'), (',', 'O'), ('said', 'O'), ('Home', 'O'), ('ministry', 'O'), ('sources', 'O'), ('on', 'O'), ('Friday', 'O'), ('.', 'O'), ('The', 'O'), ('bombs', 'O'), ('were', 'O'), ('triggered', 'O'), ('off', 'O'), ('by', 'O'), ('timer', 'O'), ('devices', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Details', 'O'), ('are', 'O'), ('awaited', 'O'), ('.', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "test_data = []\n",
    "for f in test_file_list:\n",
    "    test_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in xrange(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            test_sentences.append(pairs_in_line)\n",
    "    if len(test_sentences) > 0:\n",
    "        test_data.extend(test_sentences)\n",
    "\n",
    "print test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammonium nitrate was used to make the bombs used in the Lucknow and Faizabad blasts , said Home ministry sources on Friday . The bombs were triggered off by timer devices . . . . . Details are awaited . .\n",
      "('Predicted:', 'O O O O O O O O O O O LOC_Others O LOC_Event O O O O O O O O O O O O O O O O O O O O O O O O O O O')\n",
      "('Correct:  ', 'O O O O O O O O O O O LOC_Others O LOC_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O')\n"
     ]
    }
   ],
   "source": [
    "example_sent = test_data[0]\n",
    "print(' '.join(sent2words(example_sent)))\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2tags(example_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [sent2features(s) for s in test_data]\n",
    "y_test = [sent2tags(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 956 ms, sys: 8 ms, total: 964 ms\n",
      "Wall time: 964 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Accused       0.00      0.00      0.00        73\n",
      "      Event       0.00      0.00      0.00        51\n",
      "LOC_Accused       0.20      0.01      0.01       163\n",
      "  LOC_Assoc       0.00      0.00      0.00        51\n",
      "  LOC_Event       0.53      0.42      0.47      1151\n",
      " LOC_Others       0.58      0.54      0.56      1721\n",
      " LOC_Victim       0.00      0.00      0.00        20\n",
      "   Location       0.33      0.02      0.04        95\n",
      "ORG_Accused       0.77      0.76      0.76       572\n",
      "  ORG_Assoc       0.00      0.00      0.00        10\n",
      " ORG_Others       0.65      0.56      0.60      2091\n",
      " ORG_Victim       0.00      0.00      0.00        65\n",
      "PER_Accused       0.62      0.38      0.47       808\n",
      "  PER_Assoc       0.00      0.00      0.00        64\n",
      " PER_Others       0.72      0.70      0.71      1769\n",
      " PER_Victim       0.38      0.24      0.30       306\n",
      "     Victim       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.60      0.51      0.55      9011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "num_test_data = len(y_test)\n",
    "print len(y_pred)\n",
    "print num_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "actual_tag_count = dict()\n",
    "matched_tag_count = dict()\n",
    "matched_tag_count['PER_Others'] = 0\n",
    "actual_tag_count['PER_Others'] = 0\n",
    "matched_tag_count['PER_Victim'] = 0\n",
    "actual_tag_count['PER_Victim'] = 0\n",
    "matched_tag_count['PER_Accused'] = 0\n",
    "actual_tag_count['PER_Accused'] = 0\n",
    "matched_tag_count['ORG_Victim'] = 0\n",
    "actual_tag_count['ORG_Victim'] = 0\n",
    "matched_tag_count['ORG_Accused'] = 0\n",
    "actual_tag_count['ORG_Accused'] = 0\n",
    "matched_tag_count['ORG_Others'] = 0\n",
    "actual_tag_count['ORG_Others'] = 0\n",
    "matched_tag_count['LOC_Accused'] = 0\n",
    "actual_tag_count['LOC_Accused'] = 0\n",
    "matched_tag_count['LOC_Others'] = 0\n",
    "actual_tag_count['LOC_Others'] = 0\n",
    "matched_tag_count['LOC_Event'] = 0\n",
    "actual_tag_count['LOC_Event'] = 0\n",
    "matched_tag_count['LOC_Victim'] = 0\n",
    "actual_tag_count['LOC_Victim'] = 0\n",
    "\n",
    "for i in xrange(num_test_data):\n",
    "    y_test_i = y_test[i]\n",
    "    y_pred_i = y_pred[i]\n",
    "    test_tag_length = len(y_test[i])\n",
    "    predicted_tag_length = len(y_pred[i])\n",
    "    for j in xrange(test_tag_length):\n",
    "        if y_test_i[j] in actual_tag_count.keys():\n",
    "            actual_tag_count[y_test_i[j]] = actual_tag_count[y_test_i[j]] + 1\n",
    "        \n",
    "        if y_test_i[j] == y_pred_i[j]:\n",
    "            if y_test_i[j] in matched_tag_count.keys():\n",
    "                matched_tag_count[y_test_i[j]] = matched_tag_count[y_test_i[j]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Accuracy Class Wise =====================\n",
      "\n",
      "Class    Matched Total %\n",
      "--------------------------\n",
      "PER_Others: 1233 1769 69%\n",
      "PER_Victim: 74 306 24%\n",
      "PER_Accused: 308 808 38%\n",
      "ORG_Victim: 0 65 0%\n",
      "ORG_Accused: 433 572 75%\n",
      "ORG_Others: 1170 2091 55%\n",
      "LOC_Accused: 1 163 0%\n",
      "LOC_Others: 936 1721 54%\n",
      "LOC_Event: 481 1151 41%\n",
      "LOC_Victim: 0 20 0%\n"
     ]
    }
   ],
   "source": [
    "print('======= Accuracy Class Wise =====================\\n')\n",
    "\n",
    "print('Class    Matched Total %')\n",
    "print('--------------------------')\n",
    "print ('PER_Others: '+str(matched_tag_count['PER_Others'])+ ' ' + str(actual_tag_count['PER_Others'])+ ' ' + str(matched_tag_count['PER_Others']*100/actual_tag_count['PER_Others'])+'%')\n",
    "print ('PER_Victim: '+str(matched_tag_count['PER_Victim'])+ ' ' + str(actual_tag_count['PER_Victim'])+ ' '+str(matched_tag_count['PER_Victim']*100/actual_tag_count['PER_Victim'])+'%')\n",
    "print ('PER_Accused: '+str(matched_tag_count['PER_Accused'])+ ' ' + str(actual_tag_count['PER_Accused'])+ ' '+str(matched_tag_count['PER_Accused']*100/actual_tag_count['PER_Accused'])+'%')\n",
    "print ('ORG_Victim: '+str(matched_tag_count['ORG_Victim']) + ' '+ str(actual_tag_count['ORG_Victim'])+ ' '+str(matched_tag_count['ORG_Victim']*100/actual_tag_count['ORG_Victim'])+'%')\n",
    "print ('ORG_Accused: '+str(matched_tag_count['ORG_Accused']) + ' '+ str(actual_tag_count['ORG_Accused'])+ ' '+str(matched_tag_count['ORG_Accused']*100/actual_tag_count['ORG_Accused'])+'%')\n",
    "print ('ORG_Others: '+str(matched_tag_count['ORG_Others']) + ' '+ str(actual_tag_count['ORG_Others'])+ ' '+str(matched_tag_count['ORG_Others']*100/actual_tag_count['ORG_Others'])+'%')\n",
    "print ('LOC_Accused: '+str(matched_tag_count['LOC_Accused'])+ ' ' + str(actual_tag_count['LOC_Accused'])+ ' '+str(matched_tag_count['LOC_Accused']*100/actual_tag_count['LOC_Accused'])+'%')\n",
    "print ('LOC_Others: '+str(matched_tag_count['LOC_Others'])+ ' ' + str(actual_tag_count['LOC_Others'])+ ' '+str(matched_tag_count['LOC_Others']*100/actual_tag_count['LOC_Others'])+'%')\n",
    "print ('LOC_Event: '+str(matched_tag_count['LOC_Event']) + ' '+ str(actual_tag_count['LOC_Event'])+ ' '+str(matched_tag_count['LOC_Event']*100/actual_tag_count['LOC_Event'])+'%')\n",
    "print ('LOC_Victim: '+str(matched_tag_count['LOC_Victim']) + ' '+ str(actual_tag_count['LOC_Victim'])+ ' '+str(matched_tag_count['LOC_Victim']*100/actual_tag_count['LOC_Victim'])+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
