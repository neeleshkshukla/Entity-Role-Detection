{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import hmm\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT')]\n"
     ]
    }
   ],
   "source": [
    "train_data = treebank.tagged_sents()[:3000]\n",
    "print train_data[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for NLTK HMM Tagger\n",
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "doc_dir = 'Data/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print len(train_file_list)\n",
    "print len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Five', 'O'), ('people', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('over', 'O'), ('50', 'O'), ('injured', 'O'), ('in', 'O'), ('three', 'O'), ('blasts', 'O'), ('set', 'O'), ('off', 'O'), ('by', 'O'), ('insurgent', 'O'), ('outfit', 'O'), ('ULFA', 'ORG_Others'), ('here', 'O'), ('today', 'O'), ('hours', 'O'), ('before', 'O'), ('Union', 'O'), ('Home', 'O'), ('Minister', 'O'), ('P', 'PER_Others'), ('Chidambaram', 'PER_Others'), (\"'s\", 'O'), ('visit', 'O'), ('to', 'O'), ('review', 'O'), ('law', 'O'), ('and', 'O'), ('order', 'O'), ('situation', 'O'), ('in', 'O'), ('the', 'O'), ('state', 'O'), ('rocked', 'O'), ('by', 'O'), ('deadly', 'O'), ('blasts', 'O'), ('that', 'O'), ('left', 'O'), ('88', 'O'), ('dead', 'O'), ('two', 'O'), ('months', 'O'), ('ago', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Three', 'O'), ('people', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('35', 'O'), ('injured', 'O'), (',', 'O'), ('including', 'O'), ('four', 'O'), ('women', 'O'), (',', 'O'), ('when', 'O'), ('a', 'O'), ('bomb', 'O'), ('planted', 'O'), ('in', 'O'), ('front', 'O'), ('of', 'O'), ('a', 'O'), ('closed', 'O'), ('sweet', 'O'), ('shop', 'O'), ('exploded', 'O'), ('near', 'O'), ('upmarket', 'O'), ('Bhangagarh', 'LOC_Event'), ('flyover', 'O'), ('on', 'O'), ('the', 'O'), ('busy', 'O'), ('Guwahati-Shillong', 'O'), ('road', 'O'), ('at', 'O'), ('around', 'O'), ('5:45', 'O'), ('pm', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Two', 'O'), ('persons', 'O'), ('died', 'O'), ('on', 'O'), ('way', 'O'), ('to', 'O'), ('hospital', 'O'), ('while', 'O'), ('another', 'O'), ('succumbed', 'O'), ('to', 'O'), ('his', 'O'), ('injuries', 'O'), ('at', 'O'), ('the', 'O'), ('Guwahati', 'ORG_Others'), ('Medical', 'ORG_Others'), ('College', 'ORG_Others'), ('Hospital', 'ORG_Others'), (',', 'O'), ('DGP', 'O'), ('G', 'O'), ('M', 'O'), ('Srivastava', 'PER_Others'), ('said', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Two', 'O'), ('of', 'O'), ('the', 'O'), ('dead', 'O'), ('were', 'O'), ('identified', 'O'), ('as', 'O'), ('Amal', 'PER_Victim'), ('Das', 'PER_Victim'), ('and', 'O'), ('Kahil', 'PER_Victim'), ('Sheikh', 'PER_Victim'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('A', 'O'), ('bomb', 'O'), ('kept', 'O'), ('in', 'O'), ('a', 'O'), ('bicycle', 'O'), ('exploded', 'O'), ('at', 'O'), ('a', 'O'), ('market', 'O'), ('in', 'O'), ('the', 'O'), ('Bhootnath', 'LOC_Event'), ('area', 'O'), (',', 'O'), ('the', 'O'), ('route', 'O'), ('which', 'O'), ('Chidambaram', 'PER_Others'), ('was', 'O'), ('to', 'O'), ('take', 'O'), ('on', 'O'), ('his', 'O'), ('way', 'O'), ('from', 'O'), ('the', 'O'), ('airport', 'O'), (',', 'O'), ('under', 'O'), ('Baralumukh', 'LOC_Others'), ('police', 'O'), ('station', 'O'), ('around', 'O'), ('5.30', 'O'), ('pm', 'O'), ('in', 'O'), ('which', 'O'), ('two', 'O'), ('persons', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('12', 'O'), ('others', 'O'), ('injured', 'O'), (',', 'O'), ('official', 'O'), ('sources', 'O'), ('said', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('An', 'O'), ('Improvised', 'O'), ('Explosive', 'O'), ('Devise', 'O'), ('(', 'O'), ('IED', 'O'), (')', 'O'), ('kept', 'O'), ('in', 'O'), ('a', 'O'), ('Gauhati', 'O'), ('Municipal', 'O'), ('Corporation', 'O'), ('(', 'O'), ('GMC', 'O'), (')', 'O'), ('dustbin', 'O'), ('went', 'O'), ('off', 'O'), ('at', 'O'), ('around', 'O'), ('3.30', 'O'), ('pm', 'O'), ('injuring', 'O'), ('three', 'O'), ('persons', 'O'), ('in', 'O'), ('Birubari', 'LOC_Event'), ('Tiniali', 'LOC_Event'), ('area', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('The', 'O'), ('blasts', 'O'), ('came', 'O'), ('a', 'O'), ('day', 'O'), ('ahead', 'O'), ('of', 'O'), ('Prime', 'O'), ('Minister', 'O'), ('Manmohan', 'PER_Others'), ('Singh', 'PER_Others'), (\"'s\", 'O'), ('arrival', 'O'), ('here', 'O'), ('tomorrow', 'O'), ('enroute', 'O'), ('Shillong', 'LOC_Others'), ('to', 'O'), ('inaugurate', 'O'), ('the', 'O'), ('Indian', 'ORG_Others'), ('Science', 'ORG_Others'), ('Congress', 'ORG_Others'), ('there', 'O'), ('on', 'O'), ('January', 'O'), ('three', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Chidmabaram', 'PER_Others'), ('arrived', 'O'), ('at', 'O'), ('the', 'O'), ('Lokopriyo', 'LOC_Others'), ('Gopinath', 'LOC_Others'), ('Bordoloi', 'LOC_Others'), ('international', 'LOC_Others'), ('airport', 'LOC_Others'), ('on', 'O'), ('a', 'O'), ('two-day', 'O'), ('visit', 'O'), ('to', 'O'), ('the', 'O'), ('state', 'O'), ('to', 'O'), ('review', 'O'), ('Assam', 'LOC_Event'), (\"'s\", 'O'), ('law', 'O'), ('and', 'O'), ('order', 'O'), ('situation', 'O'), ('and', 'O'), ('attend', 'O'), ('a', 'O'), ('meeting', 'O'), ('of', 'O'), ('the', 'O'), ('Unified', 'O'), ('Command', 'O'), ('headed', 'O'), ('by', 'O'), ('Chief', 'O'), ('Minister', 'O'), ('Tarun', 'PER_Others'), ('Gogoi', 'PER_Others'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Build Tagged dataset to be fed in nltk hmm tagger\n",
    "# It is a list of tagged sentences. \n",
    "# training_data = [ [(word, tag), (word, tag).....]\n",
    "# [(word, tag), (word, tag).....]\n",
    "#]\n",
    "\n",
    "# Directory having tags\n",
    "tag_dir = 'Data/new_tags'\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for f in train_file_list:\n",
    "    training_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in xrange(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            training_sentences.append(pairs_in_line)\n",
    "    if len(training_sentences) > 0:\n",
    "        training_data.extend(training_sentences)\n",
    "\n",
    "#print training_data[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HiddenMarkovModelTagger 18 states and 20252 output symbols>\n",
      "['O', 'ORG_Others', 'PER_Others', 'LOC_Event', 'PER_Victim', 'LOC_Others', 'PER_Accused', 'ORG_Victim', 'ORG_Accused', 'PER_Assoc', 'Accused', 'LOC_Assoc', 'Event', 'Location', 'LOC_Accused', 'LOC_Victim', 'ORG_Assoc', 'Victim']\n"
     ]
    }
   ],
   "source": [
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger =  trainer.train_supervised(training_data)\n",
    "print tagger\n",
    "print tagger._states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "actual_tag_count = dict()\n",
    "matched_tag_count = dict()\n",
    "matched_tag_count['PER_Others'] = 0\n",
    "actual_tag_count['PER_Others'] = 0\n",
    "matched_tag_count['PER_Victim'] = 0\n",
    "actual_tag_count['PER_Victim'] = 0\n",
    "matched_tag_count['PER_Accused'] = 0\n",
    "actual_tag_count['PER_Accused'] = 0\n",
    "matched_tag_count['ORG_Victim'] = 0\n",
    "actual_tag_count['ORG_Victim'] = 0\n",
    "matched_tag_count['ORG_Accused'] = 0\n",
    "actual_tag_count['ORG_Accused'] = 0\n",
    "matched_tag_count['ORG_Others'] = 0\n",
    "actual_tag_count['ORG_Others'] = 0\n",
    "matched_tag_count['LOC_Accused'] = 0\n",
    "actual_tag_count['LOC_Accused'] = 0\n",
    "matched_tag_count['LOC_Others'] = 0\n",
    "actual_tag_count['LOC_Others'] = 0\n",
    "matched_tag_count['LOC_Event'] = 0\n",
    "actual_tag_count['LOC_Event'] = 0\n",
    "matched_tag_count['LOC_Victim'] = 0\n",
    "actual_tag_count['LOC_Victim'] = 0\n",
    "\n",
    "for f in test_file_list:\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    predicted_tags = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            predicted_tags = tagger.tag(line.split())\n",
    "    if len(predicted_tags) > 0:\n",
    "        actual_tags = []\n",
    "        with open(tag_file_path, \"rt\") as tag_file:\n",
    "            for line in tag_file:\n",
    "                actual_tags = line.split()\n",
    "        result_len = min(len(predicted_tags), len(actual_tags))\n",
    "        \n",
    "        for i in xrange(result_len):\n",
    "            #print predicted_tags[i][1], actual_tags[i]\n",
    "            if actual_tags[i] in actual_tag_count.keys():\n",
    "                actual_tag_count[actual_tags[i]] = actual_tag_count[actual_tags[i]] + 1\n",
    "                \n",
    "            if actual_tags[i] == predicted_tags[i][1]:\n",
    "                if actual_tags[i] in matched_tag_count.keys():\n",
    "                    matched_tag_count[actual_tags[i]] = matched_tag_count[actual_tags[i]] + 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Accuracy Class Wise =====================\n",
      "\n",
      "Class    Matched Total %\n",
      "--------------------------\n",
      "PER_Others: 246 1967 12%\n",
      "PER_Victim: 12 218 5%\n",
      "PER_Accused: 137 972 14%\n",
      "ORG_Victim: 6 60 10%\n",
      "ORG_Accused: 146 629 23%\n",
      "ORG_Others: 287 2354 12%\n",
      "LOC_Accused: 1 211 0%\n",
      "LOC_Others: 252 1849 13%\n",
      "LOC_Event: 169 1385 12%\n",
      "LOC_Victim: 0 51 0%\n"
     ]
    }
   ],
   "source": [
    "print('======= Accuracy Class Wise =====================\\n')\n",
    "\n",
    "print('Class    Matched Total %')\n",
    "print('--------------------------')\n",
    "print ('PER_Others: '+str(matched_tag_count['PER_Others'])+ ' ' + str(actual_tag_count['PER_Others'])+ ' ' + str(matched_tag_count['PER_Others']*100/actual_tag_count['PER_Others'])+'%')\n",
    "print ('PER_Victim: '+str(matched_tag_count['PER_Victim'])+ ' ' + str(actual_tag_count['PER_Victim'])+ ' '+str(matched_tag_count['PER_Victim']*100/actual_tag_count['PER_Victim'])+'%')\n",
    "print ('PER_Accused: '+str(matched_tag_count['PER_Accused'])+ ' ' + str(actual_tag_count['PER_Accused'])+ ' '+str(matched_tag_count['PER_Accused']*100/actual_tag_count['PER_Accused'])+'%')\n",
    "print ('ORG_Victim: '+str(matched_tag_count['ORG_Victim']) + ' '+ str(actual_tag_count['ORG_Victim'])+ ' '+str(matched_tag_count['ORG_Victim']*100/actual_tag_count['ORG_Victim'])+'%')\n",
    "print ('ORG_Accused: '+str(matched_tag_count['ORG_Accused']) + ' '+ str(actual_tag_count['ORG_Accused'])+ ' '+str(matched_tag_count['ORG_Accused']*100/actual_tag_count['ORG_Accused'])+'%')\n",
    "print ('ORG_Others: '+str(matched_tag_count['ORG_Others']) + ' '+ str(actual_tag_count['ORG_Others'])+ ' '+str(matched_tag_count['ORG_Others']*100/actual_tag_count['ORG_Others'])+'%')\n",
    "print ('LOC_Accused: '+str(matched_tag_count['LOC_Accused'])+ ' ' + str(actual_tag_count['LOC_Accused'])+ ' '+str(matched_tag_count['LOC_Accused']*100/actual_tag_count['LOC_Accused'])+'%')\n",
    "print ('LOC_Others: '+str(matched_tag_count['LOC_Others'])+ ' ' + str(actual_tag_count['LOC_Others'])+ ' '+str(matched_tag_count['LOC_Others']*100/actual_tag_count['LOC_Others'])+'%')\n",
    "print ('LOC_Event: '+str(matched_tag_count['LOC_Event']) + ' '+ str(actual_tag_count['LOC_Event'])+ ' '+str(matched_tag_count['LOC_Event']*100/actual_tag_count['LOC_Event'])+'%')\n",
    "print ('LOC_Victim: '+str(matched_tag_count['LOC_Victim']) + ' '+ str(actual_tag_count['LOC_Victim'])+ ' '+str(matched_tag_count['LOC_Victim']*100/actual_tag_count['LOC_Victim'])+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New', 'LOC_Others')\n"
     ]
    }
   ],
   "source": [
    "result = tagger.tag(\"New Delhi : In the wake of Fridays bomb blasts in Kabul\".split())\n",
    "print result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Dummy tab for test\n",
    "#print type(\"Chicago is the birthplace of Ginny\".split())\n",
    "x = [1,2,3]\n",
    "x.append(4)\n",
    "print x\n",
    "x.append([4,5])\n",
    "print x\n",
    "x.exten\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
