{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "\n",
    "doc_dir = '../../../../Data/input/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print (len(train_file_list))\n",
    "print (len(test_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Officials', 'O'), ('of', 'O'), ('the', 'O'), ('Special', 'O'), ('Enquiry', 'O'), ('Squad', 'O'), ('of', 'O'), ('Central', 'O'), ('Crime', 'O'), ('Branch', 'O'), ('(', 'O'), ('CCB', 'O'), (')', 'O'), ('interrogated', 'O'), ('Shahzad', 'PER_Accused'), (',', 'O'), ('an', 'O'), ('alleged', 'O'), ('Indian', 'ORG_Others'), ('Mujahideen', 'ORG_Others'), ('militant', 'O'), (',', 'O'), ('in', 'O'), ('connection', 'O'), ('with', 'O'), ('the', 'O'), ('April', 'O'), ('17', 'O'), ('Chinnaswamy', 'LOC_Event'), ('Stadium', 'LOC_Event'), ('blasts', 'O'), (',', 'O'), ('here', 'O'), ('on', 'O'), ('Friday', 'O'), ('.', 'O'), ('Shahzad', 'PER_Accused'), ('and', 'O'), ('his', 'O'), ('associate', 'O'), ('Salman', 'PER_Accused'), ('were', 'O'), ('brought', 'O'), ('here', 'O'), ('from', 'O'), ('Delhi', 'LOC_Others'), ('on', 'O'), ('Thursday', 'O'), ('.', 'O'), ('The', 'O'), ('CCB', 'O'), ('officials', 'O'), ('produced', 'O'), ('the', 'O'), ('two', 'O'), ('in', 'O'), ('the', 'O'), ('first', 'O'), ('ACMM', 'ORG_Others'), ('Court', 'ORG_Others'), ('which', 'O'), ('allowed', 'O'), ('the', 'O'), ('police', 'O'), ('to', 'O'), ('interrogate', 'O'), ('them', 'O'), ('till', 'O'), ('October', 'O'), ('30', 'O'), ('.', 'O'), ('A', 'O'), ('team', 'O'), ('headed', 'O'), ('by', 'O'), ('an', 'O'), ('Assistant', 'O'), ('Commissioner', 'O'), ('of', 'O'), ('Police', 'O'), ('interrogated', 'O'), ('Shahzad', 'PER_Accused'), ('on', 'O'), ('Friday', 'O'), ('while', 'O'), ('Salman', 'PER_Accused'), ('will', 'O'), ('be', 'O'), ('questioned', 'O'), ('on', 'O'), ('Saturday', 'O'), (',', 'O'), ('a', 'O'), ('senior', 'O'), ('police', 'O'), ('officer', 'O'), (',', 'O'), ('who', 'O'), ('wished', 'O'), ('to', 'O'), ('remain', 'O'), ('anonymous', 'O'), (',', 'O'), ('told', 'O'), ('The', 'ORG_Others'), ('Hindu', 'ORG_Others'), ('.', 'O'), ('Shahzad', 'PER_Accused'), ('was', 'O'), ('grilled', 'O'), ('about', 'O'), ('his', 'O'), ('role', 'O'), ('in', 'O'), ('the', 'O'), ('crude', 'O'), ('bomb', 'O'), ('blasts', 'O'), ('ahead', 'O'), ('of', 'O'), ('an', 'O'), ('IPL', 'ORG_Others'), ('match', 'O'), ('outside', 'O'), ('the', 'O'), ('cricket', 'O'), ('stadium', 'O'), ('and', 'O'), ('his', 'O'), ('alleged', 'O'), ('links', 'O'), ('with', 'O'), ('banned', 'O'), ('terrorist', 'O'), ('organisation', 'O'), (',', 'O'), ('Indian', 'ORG_Others'), ('Mujahideen', 'ORG_Others'), ('.', 'O'), ('OPEN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Build Tagged dataset to be fed in nltk hmm tagger\n",
    "# It is a list of tagged sentences. \n",
    "# training_data = [ [(word, tag), (word, tag).....]\n",
    "# [(word, tag), (word, tag).....]\n",
    "#]\n",
    "\n",
    "# Directory having tags\n",
    "tag_dir = '../../../../Data/input/new_tags'\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for f in train_file_list:\n",
    "    training_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in range(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in range(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            training_sentences.append(pairs_in_line)\n",
    "    if len(training_sentences) > 0:\n",
    "        training_data.extend(training_sentences)\n",
    "\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        tag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "           # '-1:tag1=' + tag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2tags(sent):\n",
    "    return [tag for word, tag in sent]\n",
    "\n",
    "def sent2words(sent):\n",
    "    return [word for word, tag in sent] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Officials', 'O'), ('of', 'O'), ('the', 'O'), ('Special', 'O'), ('Enquiry', 'O'), ('Squad', 'O'), ('of', 'O'), ('Central', 'O'), ('Crime', 'O'), ('Branch', 'O'), ('(', 'O'), ('CCB', 'O'), (')', 'O'), ('interrogated', 'O'), ('Shahzad', 'PER_Accused'), (',', 'O'), ('an', 'O'), ('alleged', 'O'), ('Indian', 'ORG_Others'), ('Mujahideen', 'ORG_Others'), ('militant', 'O'), (',', 'O'), ('in', 'O'), ('connection', 'O'), ('with', 'O'), ('the', 'O'), ('April', 'O'), ('17', 'O'), ('Chinnaswamy', 'LOC_Event'), ('Stadium', 'LOC_Event'), ('blasts', 'O'), (',', 'O'), ('here', 'O'), ('on', 'O'), ('Friday', 'O'), ('.', 'O'), ('Shahzad', 'PER_Accused'), ('and', 'O'), ('his', 'O'), ('associate', 'O'), ('Salman', 'PER_Accused'), ('were', 'O'), ('brought', 'O'), ('here', 'O'), ('from', 'O'), ('Delhi', 'LOC_Others'), ('on', 'O'), ('Thursday', 'O'), ('.', 'O'), ('The', 'O'), ('CCB', 'O'), ('officials', 'O'), ('produced', 'O'), ('the', 'O'), ('two', 'O'), ('in', 'O'), ('the', 'O'), ('first', 'O'), ('ACMM', 'ORG_Others'), ('Court', 'ORG_Others'), ('which', 'O'), ('allowed', 'O'), ('the', 'O'), ('police', 'O'), ('to', 'O'), ('interrogate', 'O'), ('them', 'O'), ('till', 'O'), ('October', 'O'), ('30', 'O'), ('.', 'O'), ('A', 'O'), ('team', 'O'), ('headed', 'O'), ('by', 'O'), ('an', 'O'), ('Assistant', 'O'), ('Commissioner', 'O'), ('of', 'O'), ('Police', 'O'), ('interrogated', 'O'), ('Shahzad', 'PER_Accused'), ('on', 'O'), ('Friday', 'O'), ('while', 'O'), ('Salman', 'PER_Accused'), ('will', 'O'), ('be', 'O'), ('questioned', 'O'), ('on', 'O'), ('Saturday', 'O'), (',', 'O'), ('a', 'O'), ('senior', 'O'), ('police', 'O'), ('officer', 'O'), (',', 'O'), ('who', 'O'), ('wished', 'O'), ('to', 'O'), ('remain', 'O'), ('anonymous', 'O'), (',', 'O'), ('told', 'O'), ('The', 'ORG_Others'), ('Hindu', 'ORG_Others'), ('.', 'O'), ('Shahzad', 'PER_Accused'), ('was', 'O'), ('grilled', 'O'), ('about', 'O'), ('his', 'O'), ('role', 'O'), ('in', 'O'), ('the', 'O'), ('crude', 'O'), ('bomb', 'O'), ('blasts', 'O'), ('ahead', 'O'), ('of', 'O'), ('an', 'O'), ('IPL', 'ORG_Others'), ('match', 'O'), ('outside', 'O'), ('the', 'O'), ('cricket', 'O'), ('stadium', 'O'), ('and', 'O'), ('his', 'O'), ('alleged', 'O'), ('links', 'O'), ('with', 'O'), ('banned', 'O'), ('terrorist', 'O'), ('organisation', 'O'), (',', 'O'), ('Indian', 'ORG_Others'), ('Mujahideen', 'ORG_Others'), ('.', 'O'), ('OPEN', 'O')]\n",
      "['bias', 'word.lower=officials', 'word[-3:]=als', 'word[-2:]=ls', 'word.isupper=False', 'word.istitle=True', 'word.isdigit=False', 'BOS', '+1:word.lower=of', '+1:word.istitle=False', '+1:word.isupper=False']\n",
      "O\n",
      "Officials\n"
     ]
    }
   ],
   "source": [
    "print (training_data[0])\n",
    "print (sent2features(training_data[0])[0])\n",
    "print (sent2tags(training_data[0])[0])\n",
    "print (sent2words(training_data[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sent2features(s) for s in training_data]\n",
    "y_train = [sent2tags(s) for s in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the model\n",
    "trainer.train('osint-bomb-blast.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f6833ef94a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('osint-bomb-blast.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('Karnataka', 'LOC_Event'), ('Police', 'O'), ('have', 'O'), ('arrested', 'O'), ('Ibrahim', 'PER_Accused'), ('Moulavi', 'PER_Accused'), (',', 'O'), ('an', 'O'), ('accused', 'O'), ('in', 'O'), ('the', 'O'), ('Bangalore', 'LOC_Event'), ('bomb', 'O'), ('blasts', 'O'), ('case', 'O'), (',', 'O'), ('near', 'O'), ('Kasaragod', 'LOC_Event'), ('.', 'O'), ('Page', 'O'), ('4', 'O'), ('OPEN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "test_data = []\n",
    "for f in test_file_list:\n",
    "    test_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in range(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in range(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            test_sentences.append(pairs_in_line)\n",
    "    if len(test_sentences) > 0:\n",
    "        test_data.extend(test_sentences)\n",
    "\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Karnataka Police have arrested Ibrahim Moulavi , an accused in the Bangalore bomb blasts case , near Kasaragod . Page 4 OPEN\n",
      "Predicted: O ORG_Others ORG_Others O O PER_Others PER_Others O O O O O LOC_Event O O O O O LOC_Others O O O O\n",
      "Correct:   O LOC_Event O O O PER_Accused PER_Accused O O O O O LOC_Event O O O O O LOC_Event O O O O\n"
     ]
    }
   ],
   "source": [
    "example_sent = test_data[0]\n",
    "print(' '.join(sent2words(example_sent)))\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2tags(example_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [sent2features(s) for s in test_data]\n",
    "y_test = [sent2tags(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 752 ms, sys: 0 ns, total: 752 ms\n",
      "Wall time: 756 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "LOC_Accused       0.35      0.09      0.14        80\n",
      "  LOC_Event       0.57      0.44      0.50      1168\n",
      " LOC_Others       0.59      0.62      0.60      1625\n",
      " LOC_Victim       0.33      0.05      0.09        41\n",
      "ORG_Accused       0.82      0.82      0.82       528\n",
      " ORG_Others       0.71      0.65      0.68      1957\n",
      " ORG_Victim       0.15      0.07      0.09        45\n",
      "PER_Accused       0.61      0.58      0.59       504\n",
      " PER_Others       0.80      0.77      0.78      1669\n",
      " PER_Victim       0.53      0.27      0.36       211\n",
      "\n",
      "avg / total       0.67      0.62      0.64      7828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "num_test_data = len(y_test)\n",
    "print (len(y_pred))\n",
    "print (num_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "actual_tag_count = dict()\n",
    "matched_tag_count = dict()\n",
    "matched_tag_count['PER_Others'] = 0\n",
    "actual_tag_count['PER_Others'] = 0\n",
    "matched_tag_count['PER_Victim'] = 0\n",
    "actual_tag_count['PER_Victim'] = 0\n",
    "matched_tag_count['PER_Accused'] = 0\n",
    "actual_tag_count['PER_Accused'] = 0\n",
    "matched_tag_count['ORG_Victim'] = 0\n",
    "actual_tag_count['ORG_Victim'] = 0\n",
    "matched_tag_count['ORG_Accused'] = 0\n",
    "actual_tag_count['ORG_Accused'] = 0\n",
    "matched_tag_count['ORG_Others'] = 0\n",
    "actual_tag_count['ORG_Others'] = 0\n",
    "matched_tag_count['LOC_Accused'] = 0\n",
    "actual_tag_count['LOC_Accused'] = 0\n",
    "matched_tag_count['LOC_Others'] = 0\n",
    "actual_tag_count['LOC_Others'] = 0\n",
    "matched_tag_count['LOC_Event'] = 0\n",
    "actual_tag_count['LOC_Event'] = 0\n",
    "matched_tag_count['LOC_Victim'] = 0\n",
    "actual_tag_count['LOC_Victim'] = 0\n",
    "\n",
    "for i in range(num_test_data):\n",
    "    y_test_i = y_test[i]\n",
    "    y_pred_i = y_pred[i]\n",
    "    test_tag_length = len(y_test[i])\n",
    "    predicted_tag_length = len(y_pred[i])\n",
    "    for j in range(test_tag_length):\n",
    "        if y_test_i[j] in actual_tag_count.keys():\n",
    "            actual_tag_count[y_test_i[j]] = actual_tag_count[y_test_i[j]] + 1\n",
    "        \n",
    "        if y_test_i[j] == y_pred_i[j]:\n",
    "            if y_test_i[j] in matched_tag_count.keys():\n",
    "                matched_tag_count[y_test_i[j]] = matched_tag_count[y_test_i[j]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Accuracy Class Wise =====================\n",
      "\n",
      "Class    Matched Total %\n",
      "--------------------------\n",
      "PER_Others: 1278 1669 76.57279808268424%\n",
      "PER_Victim: 58 211 27.488151658767773%\n",
      "PER_Accused: 291 504 57.73809523809524%\n",
      "ORG_Victim: 3 45 6.666666666666667%\n",
      "ORG_Accused: 431 528 81.62878787878788%\n",
      "ORG_Others: 1277 1957 65.2529381706694%\n",
      "LOC_Accused: 7 80 8.75%\n",
      "LOC_Others: 1000 1625 61.53846153846154%\n",
      "LOC_Event: 517 1168 44.263698630136986%\n",
      "LOC_Victim: 2 41 4.878048780487805%\n"
     ]
    }
   ],
   "source": [
    "print('======= Accuracy Class Wise =====================\\n')\n",
    "\n",
    "print('Class    Matched Total %')\n",
    "print('--------------------------')\n",
    "print ('PER_Others: '+str(matched_tag_count['PER_Others'])+ ' ' + str(actual_tag_count['PER_Others'])+ ' ' + str(matched_tag_count['PER_Others']*100/actual_tag_count['PER_Others'])+'%')\n",
    "print ('PER_Victim: '+str(matched_tag_count['PER_Victim'])+ ' ' + str(actual_tag_count['PER_Victim'])+ ' '+str(matched_tag_count['PER_Victim']*100/actual_tag_count['PER_Victim'])+'%')\n",
    "print ('PER_Accused: '+str(matched_tag_count['PER_Accused'])+ ' ' + str(actual_tag_count['PER_Accused'])+ ' '+str(matched_tag_count['PER_Accused']*100/actual_tag_count['PER_Accused'])+'%')\n",
    "print ('ORG_Victim: '+str(matched_tag_count['ORG_Victim']) + ' '+ str(actual_tag_count['ORG_Victim'])+ ' '+str(matched_tag_count['ORG_Victim']*100/actual_tag_count['ORG_Victim'])+'%')\n",
    "print ('ORG_Accused: '+str(matched_tag_count['ORG_Accused']) + ' '+ str(actual_tag_count['ORG_Accused'])+ ' '+str(matched_tag_count['ORG_Accused']*100/actual_tag_count['ORG_Accused'])+'%')\n",
    "print ('ORG_Others: '+str(matched_tag_count['ORG_Others']) + ' '+ str(actual_tag_count['ORG_Others'])+ ' '+str(matched_tag_count['ORG_Others']*100/actual_tag_count['ORG_Others'])+'%')\n",
    "print ('LOC_Accused: '+str(matched_tag_count['LOC_Accused'])+ ' ' + str(actual_tag_count['LOC_Accused'])+ ' '+str(matched_tag_count['LOC_Accused']*100/actual_tag_count['LOC_Accused'])+'%')\n",
    "print ('LOC_Others: '+str(matched_tag_count['LOC_Others'])+ ' ' + str(actual_tag_count['LOC_Others'])+ ' '+str(matched_tag_count['LOC_Others']*100/actual_tag_count['LOC_Others'])+'%')\n",
    "print ('LOC_Event: '+str(matched_tag_count['LOC_Event']) + ' '+ str(actual_tag_count['LOC_Event'])+ ' '+str(matched_tag_count['LOC_Event']*100/actual_tag_count['LOC_Event'])+'%')\n",
    "print ('LOC_Victim: '+str(matched_tag_count['LOC_Victim']) + ' '+ str(actual_tag_count['LOC_Victim'])+ ' '+str(matched_tag_count['LOC_Victim']*100/actual_tag_count['LOC_Victim'])+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
