{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "# To test for PER, LOC and ORG Only\n",
    "\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "# Divide in train and test files [80:20] \n",
    "\n",
    "# Directory having content\n",
    "\n",
    "doc_dir = 'Data/content'\n",
    "\n",
    "train_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "for f in os.listdir(doc_dir):\n",
    "    #Random Sampling\n",
    "    if np.random.uniform(0,1)< 0.8:\n",
    "        train_file_list.append(f)\n",
    "    else:\n",
    "        test_file_list.append(f)\n",
    "\n",
    "print len(train_file_list)\n",
    "print len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Tagged dataset to be fed in nltk hmm tagger\n",
    "# It is a list of tagged sentences. \n",
    "# training_data = [ [(word, tag), (word, tag).....]\n",
    "# [(word, tag), (word, tag).....]\n",
    "#]\n",
    "\n",
    "# Directory having tags\n",
    "tag_dir = 'Data/new_tags'\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for f in train_file_list:\n",
    "    training_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in xrange(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            training_sentences.append(pairs_in_line)\n",
    "    if len(training_sentences) > 0:\n",
    "        training_data.extend(training_sentences)\n",
    "\n",
    "#print training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        tag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "           # '-1:tag1=' + tag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2tags(sent):\n",
    "    return [tag for word, tag in sent]\n",
    "\n",
    "def sent2words(sent):\n",
    "    return [word for word, tag in sent] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Five', 'O'), ('people', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('over', 'O'), ('50', 'O'), ('injured', 'O'), ('in', 'O'), ('three', 'O'), ('blasts', 'O'), ('set', 'O'), ('off', 'O'), ('by', 'O'), ('insurgent', 'O'), ('outfit', 'O'), ('ULFA', 'ORG_Others'), ('here', 'O'), ('today', 'O'), ('hours', 'O'), ('before', 'O'), ('Union', 'O'), ('Home', 'O'), ('Minister', 'O'), ('P', 'PER_Others'), ('Chidambaram', 'PER_Others'), (\"'s\", 'O'), ('visit', 'O'), ('to', 'O'), ('review', 'O'), ('law', 'O'), ('and', 'O'), ('order', 'O'), ('situation', 'O'), ('in', 'O'), ('the', 'O'), ('state', 'O'), ('rocked', 'O'), ('by', 'O'), ('deadly', 'O'), ('blasts', 'O'), ('that', 'O'), ('left', 'O'), ('88', 'O'), ('dead', 'O'), ('two', 'O'), ('months', 'O'), ('ago', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Three', 'O'), ('people', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('35', 'O'), ('injured', 'O'), (',', 'O'), ('including', 'O'), ('four', 'O'), ('women', 'O'), (',', 'O'), ('when', 'O'), ('a', 'O'), ('bomb', 'O'), ('planted', 'O'), ('in', 'O'), ('front', 'O'), ('of', 'O'), ('a', 'O'), ('closed', 'O'), ('sweet', 'O'), ('shop', 'O'), ('exploded', 'O'), ('near', 'O'), ('upmarket', 'O'), ('Bhangagarh', 'LOC_Event'), ('flyover', 'O'), ('on', 'O'), ('the', 'O'), ('busy', 'O'), ('Guwahati-Shillong', 'O'), ('road', 'O'), ('at', 'O'), ('around', 'O'), ('5:45', 'O'), ('pm', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Two', 'O'), ('persons', 'O'), ('died', 'O'), ('on', 'O'), ('way', 'O'), ('to', 'O'), ('hospital', 'O'), ('while', 'O'), ('another', 'O'), ('succumbed', 'O'), ('to', 'O'), ('his', 'O'), ('injuries', 'O'), ('at', 'O'), ('the', 'O'), ('Guwahati', 'ORG_Others'), ('Medical', 'ORG_Others'), ('College', 'ORG_Others'), ('Hospital', 'ORG_Others'), (',', 'O'), ('DGP', 'O'), ('G', 'O'), ('M', 'O'), ('Srivastava', 'PER_Others'), ('said', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Two', 'O'), ('of', 'O'), ('the', 'O'), ('dead', 'O'), ('were', 'O'), ('identified', 'O'), ('as', 'O'), ('Amal', 'PER_Victim'), ('Das', 'PER_Victim'), ('and', 'O'), ('Kahil', 'PER_Victim'), ('Sheikh', 'PER_Victim'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('A', 'O'), ('bomb', 'O'), ('kept', 'O'), ('in', 'O'), ('a', 'O'), ('bicycle', 'O'), ('exploded', 'O'), ('at', 'O'), ('a', 'O'), ('market', 'O'), ('in', 'O'), ('the', 'O'), ('Bhootnath', 'LOC_Event'), ('area', 'O'), (',', 'O'), ('the', 'O'), ('route', 'O'), ('which', 'O'), ('Chidambaram', 'PER_Others'), ('was', 'O'), ('to', 'O'), ('take', 'O'), ('on', 'O'), ('his', 'O'), ('way', 'O'), ('from', 'O'), ('the', 'O'), ('airport', 'O'), (',', 'O'), ('under', 'O'), ('Baralumukh', 'LOC_Others'), ('police', 'O'), ('station', 'O'), ('around', 'O'), ('5.30', 'O'), ('pm', 'O'), ('in', 'O'), ('which', 'O'), ('two', 'O'), ('persons', 'O'), ('were', 'O'), ('killed', 'O'), ('and', 'O'), ('12', 'O'), ('others', 'O'), ('injured', 'O'), (',', 'O'), ('official', 'O'), ('sources', 'O'), ('said', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('An', 'O'), ('Improvised', 'O'), ('Explosive', 'O'), ('Devise', 'O'), ('(', 'O'), ('IED', 'O'), (')', 'O'), ('kept', 'O'), ('in', 'O'), ('a', 'O'), ('Gauhati', 'O'), ('Municipal', 'O'), ('Corporation', 'O'), ('(', 'O'), ('GMC', 'O'), (')', 'O'), ('dustbin', 'O'), ('went', 'O'), ('off', 'O'), ('at', 'O'), ('around', 'O'), ('3.30', 'O'), ('pm', 'O'), ('injuring', 'O'), ('three', 'O'), ('persons', 'O'), ('in', 'O'), ('Birubari', 'LOC_Event'), ('Tiniali', 'LOC_Event'), ('area', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('The', 'O'), ('blasts', 'O'), ('came', 'O'), ('a', 'O'), ('day', 'O'), ('ahead', 'O'), ('of', 'O'), ('Prime', 'O'), ('Minister', 'O'), ('Manmohan', 'PER_Others'), ('Singh', 'PER_Others'), (\"'s\", 'O'), ('arrival', 'O'), ('here', 'O'), ('tomorrow', 'O'), ('enroute', 'O'), ('Shillong', 'LOC_Others'), ('to', 'O'), ('inaugurate', 'O'), ('the', 'O'), ('Indian', 'ORG_Others'), ('Science', 'ORG_Others'), ('Congress', 'ORG_Others'), ('there', 'O'), ('on', 'O'), ('January', 'O'), ('three', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('Chidmabaram', 'PER_Others'), ('arrived', 'O'), ('at', 'O'), ('the', 'O'), ('Lokopriyo', 'LOC_Others'), ('Gopinath', 'LOC_Others'), ('Bordoloi', 'LOC_Others'), ('international', 'LOC_Others'), ('airport', 'LOC_Others'), ('on', 'O'), ('a', 'O'), ('two-day', 'O'), ('visit', 'O'), ('to', 'O'), ('the', 'O'), ('state', 'O'), ('to', 'O'), ('review', 'O'), ('Assam', 'LOC_Event'), (\"'s\", 'O'), ('law', 'O'), ('and', 'O'), ('order', 'O'), ('situation', 'O'), ('and', 'O'), ('attend', 'O'), ('a', 'O'), ('meeting', 'O'), ('of', 'O'), ('the', 'O'), ('Unified', 'O'), ('Command', 'O'), ('headed', 'O'), ('by', 'O'), ('Chief', 'O'), ('Minister', 'O'), ('Tarun', 'PER_Others'), ('Gogoi', 'PER_Others'), ('.', 'O'), ('.', 'O'), ('.', 'O'), ('.', 'O')]\n",
      "['bias', 'word.lower=five', 'word[-3:]=ive', 'word[-2:]=ve', 'word.isupper=False', 'word.istitle=True', 'word.isdigit=False', 'BOS', '+1:word.lower=people', '+1:word.istitle=False', '+1:word.isupper=False']\n",
      "O\n",
      "Five\n"
     ]
    }
   ],
   "source": [
    "print training_data[0]\n",
    "print sent2features(training_data[0])[0]\n",
    "print sent2tags(training_data[0])[0]\n",
    "print sent2words(training_data[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sent2features(s) for s in training_data]\n",
    "y_train = [sent2tags(s) for s in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the model\n",
    "trainer.train('osint-bomb-blast.crfsuite.topcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f4f51040a10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('osint-bomb-blast.crfsuite.topcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('Maharashtra', 'ORG_Others'), ('Anti-Terrorism', 'ORG_Others'), ('Squad', 'ORG_Others'), ('has', 'O'), ('finally', 'O'), ('established', 'O'), ('the', 'O'), ('Sri', 'LOC_Others'), ('Lankan', 'LOC_Others'), ('connection', 'O'), ('in', 'O'), ('the', 'O'), ('Pune', 'LOC_Others'), ('German', 'LOC_Event'), ('Bakery', 'LOC_Event'), ('blast', 'O'), ('case', 'O'), ('.', 'O'), ('Mirza', 'PER_Accused'), ('Himayat', 'PER_Accused'), ('Inayat', 'PER_Accused'), ('Baig', 'PER_Accused'), (',', 'O'), ('the', 'O'), ('alleged', 'O'), ('mastermind', 'O'), (',', 'O'), ('underwent', 'O'), ('training', 'O'), ('to', 'O'), ('assemble', 'O'), ('explosive', 'O'), ('devices', 'O'), ('and', 'O'), ('cause', 'O'), ('their', 'O'), ('explosion', 'O'), (\"'\", 'O'), ('in', 'O'), ('Colombo', 'LOC_Others'), ('in', 'O'), ('March', 'O'), ('2008', 'O'), (',', 'O'), ('says', 'O'), ('the', 'O'), ('charge', 'O'), ('sheet', 'O'), ('filed', 'O'), ('by', 'O'), ('the', 'O'), ('ATS', 'O'), ('against', 'O'), ('Baig', 'PER_Accused'), ('and', 'O'), ('six', 'O'), ('other', 'O'), ('accused', 'O'), ('.', 'O'), ('He', 'O'), ('was', 'O'), ('trained', 'O'), ('by', 'O'), ('two', 'O'), ('of', 'O'), ('the', 'O'), ('accused', 'O'), (',', 'O'), ('Faiyaz', 'PER_Accused'), ('Kagzi', 'PER_Accused'), ('and', 'O'), ('Zabiuddin', 'PER_Accused'), ('Ansari', 'PER_Accused'), ('.', 'O'), ('The', 'O'), ('charge', 'O'), ('sheet', 'O'), ('says', 'O'), ('Kagzi', 'PER_Accused'), ('and', 'O'), ('Ansari', 'PER_Accused'), ('gave', 'O'), ('Baig', 'PER_Accused'), ('money', 'O'), ('or', 'O'), ('funding', 'O'), ('the', 'O'), ('purchase', 'O'), ('of', 'O'), ('explosive', 'O'), ('devices', 'O'), ('and', 'O'), ('the', 'O'), ('travel', 'O'), ('of', 'O'), ('indoctrinated', 'O'), ('Muslim', 'O'), ('youth', 'O'), ('to', 'O'), ('Pakistan', 'LOC_Others'), ('for', 'O'), ('training', 'O'), ('.', 'O'), ('After', 'O'), ('training', 'O'), ('in', 'O'), ('Pakistan', 'LOC_Others'), (',', 'O'), ('Baig', 'PER_Accused'), ('returned', 'O'), ('to', 'O'), ('India', 'LOC_Others'), ('and', 'O'), ('lived', 'O'), ('in', 'O'), ('the', 'O'), ('non-descript', 'O'), ('Udgir', 'LOC_Others'), ('taluk', 'O'), ('in', 'O'), ('Maharashtra', 'LOC_Others'), (\"'s\", 'O'), ('Latur', 'LOC_Others'), ('district', 'O'), ('under', 'O'), ('concealed', 'O'), ('identity', 'O'), ('as', 'O'), ('Yusuf', 'PER_Accused'), (\"'\", 'O'), ('and', 'O'), ('Hasan', 'PER_Accused'), (\"'\", 'O'), ('.', 'O'), ('In', 'O'), ('the', 'O'), ('last', 'O'), ('week', 'O'), ('of', 'O'), ('January', 'O'), ('2010', 'O'), (',', 'O'), ('on', 'O'), ('instructions', 'O'), ('from', 'O'), ('two', 'O'), ('of', 'O'), ('the', 'O'), ('accused', 'O'), ('Riyaz', 'PER_Accused'), ('Bhatkal', 'PER_Accused'), ('and', 'O'), ('Iqbal', 'PER_Accused'), ('Bhatkal', 'PER_Accused'), (',', 'O'), ('the', 'O'), ('other', 'O'), ('two', 'O'), ('accused', 'O'), ('Yasin', 'PER_Accused'), ('Bhatkal', 'PER_Accused'), ('and', 'O'), ('Mohsin', 'PER_Accused'), ('Chowdhary', 'PER_Accused'), ('met', 'O'), ('Baig', 'PER_Accused'), ('in', 'O'), ('Udgir', 'LOC_Others'), ('to', 'O'), ('discuss', 'O'), (',', 'O'), ('finalise', 'O'), ('and', 'O'), ('put', 'O'), ('into', 'O'), ('execution', 'O'), ('the', 'O'), ('plan', 'O'), ('to', 'O'), ('cause', 'O'), ('the', 'O'), ('bomb', 'O'), ('explosion', 'O'), ('at', 'O'), ('German', 'ORG_Victim'), ('Bakery', 'ORG_Victim'), (',', 'O'), ('according', 'O'), ('to', 'O'), ('the', 'O'), ('charge', 'O'), ('sheet', 'O'), ('.', 'O'), ('On', 'O'), ('January', 'O'), ('31', 'O'), (',', 'O'), ('Baig', 'PER_Accused'), ('conducted', 'O'), ('a', 'O'), ('recce', 'O'), ('and', 'O'), ('decided', 'O'), ('the', 'O'), ('timing', 'O'), ('of', 'O'), ('planting', 'O'), ('the', 'O'), ('explosive', 'O'), ('device', 'O'), ('.', 'O'), ('In', 'O'), ('the', 'O'), ('first', 'O'), ('week', 'O'), ('of', 'O'), ('February', 'O'), (',', 'O'), ('Yasin', 'PER_Accused'), ('and', 'O'), ('Chowdhary', 'PER_Accused'), ('went', 'O'), ('to', 'O'), ('Udgir', 'LOC_Others'), ('again', 'O'), ('to', 'O'), ('give', 'O'), ('finishing', 'O'), ('touches', 'O'), ('to', 'O'), ('the', 'O'), ('plan', 'O'), ('.', 'O'), ('Thereafter', 'O'), (',', 'O'), ('the', 'O'), ('duo', 'O'), ('accompanied', 'O'), ('Baig', 'PER_Accused'), ('to', 'O'), ('Mumbai', 'LOC_Others'), (',', 'O'), ('where', 'O'), ('a', 'O'), ('haversack', 'O'), ('and', 'O'), ('a', 'O'), ('Nokia', 'O'), ('1100', 'O'), ('model', 'O'), ('mobile', 'O'), ('phone', 'O'), ('(', 'O'), ('which', 'O'), ('was', 'O'), ('used', 'O'), ('to', 'O'), ('trigger', 'O'), ('the', 'O'), ('explosion', 'O'), (')', 'O'), ('were', 'O'), ('bought', 'O'), ('.', 'O'), ('Yasin', 'PER_Accused'), ('and', 'O'), ('Chowdhary', 'PER_Accused'), ('assembled', 'O'), ('the', 'O'), ('explosive', 'O'), ('device', 'O'), ('at', 'O'), ('the', 'O'), ('Global', 'ORG_Others'), ('Internet', 'ORG_Others'), ('Caf', 'ORG_Others'), ('in', 'O'), ('Udgir', 'LOC_Others'), ('.', 'O'), ('The', 'O'), ('charge', 'O'), ('sheet', 'O'), ('says', 'O'), ('the', 'O'), ('three', 'O'), ('conspirators', 'O'), ('then', 'O'), ('travelled', 'O'), ('to', 'O'), ('Latur', 'LOC_Others'), (',', 'O'), ('taking', 'O'), ('private', 'O'), ('transport', 'O'), (',', 'O'), ('and', 'O'), ('again', 'O'), ('from', 'O'), ('Udgir', 'LOC_Others'), ('to', 'O'), ('Latur', 'LOC_Others'), ('in', 'O'), ('a', 'O'), ('state', 'O'), ('corporation', 'O'), ('bus', 'O'), ('on', 'O'), ('February', 'O'), ('13', 'O'), (',', 'O'), ('the', 'O'), ('day', 'O'), ('of', 'O'), ('the', 'O'), ('blast', 'O'), ('.', 'O'), ('Yasin', 'PER_Accused'), ('proceeded', 'O'), ('to', 'O'), ('the', 'O'), ('German', 'ORG_Victim'), ('Bakery', 'ORG_Victim'), ('and', 'O'), ('planted', 'O'), ('the', 'O'), ('bomb', 'O'), ('at', 'O'), ('about', 'O'), ('5', 'O'), ('p.m.', 'O'), (',', 'O'), ('and', 'O'), ('it', 'O'), ('went', 'O'), ('off', 'O'), ('at', 'O'), ('6.50', 'O'), ('p.m.', 'O'), (',', 'O'), ('killing', 'O'), ('17', 'O'), ('people', 'O'), ('.', 'O'), ('All', 'O'), ('the', 'O'), ('accused', 'O'), (',', 'O'), ('including', 'O'), ('Baig', 'PER_Accused'), (',', 'O'), ('are', 'O'), ('part', 'O'), ('of', 'O'), ('the', 'O'), ('banned', 'O'), ('Students', 'ORG_Accused'), ('Islamic', 'ORG_Accused'), ('Movement', 'ORG_Accused'), ('of', 'ORG_Accused'), ('India', 'ORG_Accused'), ('and', 'O'), ('the', 'O'), ('Lashkar-e-Taiba', 'ORG_Accused'), (',', 'O'), ('the', 'O'), ('charge', 'O'), ('sheet', 'O'), ('states', 'O'), ('.', 'O'), ('Baig', 'PER_Accused'), ('is', 'O'), ('also', 'O'), ('believed', 'O'), ('to', 'O'), ('be', 'O'), ('an', 'O'), ('operator', 'O'), ('of', 'O'), ('the', 'O'), ('terror', 'O'), ('outfit', 'O'), (',', 'O'), ('Indian', 'ORG_Accused'), ('Mujahideen', 'ORG_Accused'), ('.', 'O'), ('The', 'O'), ('ATS', 'O'), ('contends', 'O'), ('that', 'O'), ('Baig', 'PER_Accused'), ('had', 'O'), ('used', 'O'), ('his', 'O'), ('friend', 'O'), ('Rehman', 'PER_Accused'), (\"'s\", 'O'), ('bank', 'O'), ('account', 'O'), ('to', 'O'), ('make', 'O'), ('various', 'O'), ('transactions', 'O'), ('from', 'O'), ('Udgir', 'LOC_Others'), ('.', 'O'), ('He', 'O'), ('used', 'O'), ('25', 'O'), ('e-mail', 'O'), ('IDs', 'O'), ('to', 'O'), ('communicate', 'O'), ('with', 'O'), ('the', 'O'), ('other', 'O'), ('accused', 'O'), (',', 'O'), ('while', 'O'), ('the', 'O'), ('plan', 'O'), ('was', 'O'), ('being', 'O'), ('hatched', 'O'), ('.', 'O'), ('In', 'O'), ('pursuance', 'O'), ('of', 'O'), ('the', 'O'), ('conspiracy', 'O'), (',', 'O'), (\"'\", 'O'), ('Baig', 'PER_Accused'), ('acquired', 'O'), ('forged', 'O'), ('voter', 'O'), ('cards', 'O'), ('under', 'O'), ('two', 'O'), ('names', 'O'), (',', 'O'), ('Khayum', 'PER_Accused'), ('Ayub', 'PER_Accused'), ('Shaikh', 'PER_Accused'), ('and', 'O'), ('Immadoddin', 'PER_Accused'), ('Ahmed', 'PER_Accused'), ('Shaikh', 'PER_Accused'), (',', 'O'), ('the', 'O'), ('charge', 'O'), ('sheet', 'O'), ('states', 'O'), ('.', 'O'), ('He', 'O'), ('had', 'O'), ('even', 'O'), ('a', 'O'), ('forged', 'O'), ('handicapped', 'O'), ('person', 'O'), ('card', 'O'), ('.', 'O'), ('The', 'O'), ('Latur', 'ORG_Others'), ('police', 'ORG_Others'), ('said', 'O'), ('Khurshid', 'PER_Accused'), ('Alam', 'PER_Accused'), (',', 'O'), ('who', 'O'), ('allegedly', 'O'), ('helped', 'O'), ('Baig', 'PER_Accused'), ('acquire', 'O'), ('the', 'O'), ('forged', 'O'), ('voter', 'O'), ('card', 'O'), ('under', 'O'), ('the', 'O'), ('name', 'O'), ('of', 'O'), ('Khayum', 'PER_Accused'), ('Ayub', 'PER_Accused'), ('Shaikh', 'PER_Accused'), (',', 'O'), ('is', 'O'), ('on', 'O'), ('the', 'O'), ('run', 'O'), ('.', 'O'), ('Falsely', 'O'), ('implicated', 'O'), ('Commenting', 'O'), ('on', 'O'), ('the', 'O'), ('charge', 'O'), ('sheet', 'O'), (',', 'O'), ('Baig', 'PER_Accused'), (\"'s\", 'O'), ('lawyer', 'O'), ('A.', 'PER_Others'), ('Rehman', 'PER_Others'), ('said', 'O'), ('his', 'O'), ('client', 'O'), ('was', 'O'), ('falsely', 'O'), ('implicated', 'O'), ('.', 'O'), ('Baig', 'PER_Accused'), ('was', 'O'), ('given', 'O'), ('the', 'O'), ('charge', 'O'), ('sheet', 'O'), ('on', 'O'), ('Monday', 'O'), ('in', 'O'), ('the', 'O'), ('first', 'O'), ('class', 'O'), ('judicial', 'O'), ('magistrate', 'O'), ('court', 'O'), ('.', 'O'), ('The', 'O'), ('case', 'O'), ('is', 'O'), ('now', 'O'), ('moved', 'O'), ('to', 'O'), ('the', 'O'), ('sessions', 'O'), ('.', 'O'), ('OPEN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "test_data = []\n",
    "for f in test_file_list:\n",
    "    test_sentences =[]\n",
    "    word_file_path = os.path.join(doc_dir, f)\n",
    "    tag_file_path = os.path.join(tag_dir, f)\n",
    "    lines_in_word_file = []\n",
    "    lines_in_tag_file = []\n",
    "    with open(word_file_path, \"rt\") as word_file:\n",
    "        for line in word_file:\n",
    "            lines_in_word_file.append(line)\n",
    "    with open(tag_file_path, \"rt\") as tag_file:\n",
    "        for line in tag_file:\n",
    "            lines_in_tag_file.append(line)\n",
    "    if (len(lines_in_word_file) == len(lines_in_tag_file)) and len(lines_in_word_file) > 0:\n",
    "        for i in xrange(len(lines_in_word_file)):\n",
    "            word_in_file = lines_in_word_file[i].split()\n",
    "            tag_in_file = lines_in_tag_file[i].split()\n",
    "            pairs_in_line = []\n",
    "            length = min(len(word_in_file), len(tag_in_file))\n",
    "            #Create the word_tag pair\n",
    "            for j in xrange(length):\n",
    "                pairs_in_line.append((word_in_file[j], tag_in_file[j]));\n",
    "            test_sentences.append(pairs_in_line)\n",
    "    if len(test_sentences) > 0:\n",
    "        test_data.extend(test_sentences)\n",
    "\n",
    "print test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Maharashtra Anti-Terrorism Squad has finally established the Sri Lankan connection in the Pune German Bakery blast case . Mirza Himayat Inayat Baig , the alleged mastermind , underwent training to assemble explosive devices and cause their explosion ' in Colombo in March 2008 , says the charge sheet filed by the ATS against Baig and six other accused . He was trained by two of the accused , Faiyaz Kagzi and Zabiuddin Ansari . The charge sheet says Kagzi and Ansari gave Baig money or funding the purchase of explosive devices and the travel of indoctrinated Muslim youth to Pakistan for training . After training in Pakistan , Baig returned to India and lived in the non-descript Udgir taluk in Maharashtra 's Latur district under concealed identity as Yusuf ' and Hasan ' . In the last week of January 2010 , on instructions from two of the accused Riyaz Bhatkal and Iqbal Bhatkal , the other two accused Yasin Bhatkal and Mohsin Chowdhary met Baig in Udgir to discuss , finalise and put into execution the plan to cause the bomb explosion at German Bakery , according to the charge sheet . On January 31 , Baig conducted a recce and decided the timing of planting the explosive device . In the first week of February , Yasin and Chowdhary went to Udgir again to give finishing touches to the plan . Thereafter , the duo accompanied Baig to Mumbai , where a haversack and a Nokia 1100 model mobile phone ( which was used to trigger the explosion ) were bought . Yasin and Chowdhary assembled the explosive device at the Global Internet Caf in Udgir . The charge sheet says the three conspirators then travelled to Latur , taking private transport , and again from Udgir to Latur in a state corporation bus on February 13 , the day of the blast . Yasin proceeded to the German Bakery and planted the bomb at about 5 p.m. , and it went off at 6.50 p.m. , killing 17 people . All the accused , including Baig , are part of the banned Students Islamic Movement of India and the Lashkar-e-Taiba , the charge sheet states . Baig is also believed to be an operator of the terror outfit , Indian Mujahideen . The ATS contends that Baig had used his friend Rehman 's bank account to make various transactions from Udgir . He used 25 e-mail IDs to communicate with the other accused , while the plan was being hatched . In pursuance of the conspiracy , ' Baig acquired forged voter cards under two names , Khayum Ayub Shaikh and Immadoddin Ahmed Shaikh , the charge sheet states . He had even a forged handicapped person card . The Latur police said Khurshid Alam , who allegedly helped Baig acquire the forged voter card under the name of Khayum Ayub Shaikh , is on the run . Falsely implicated Commenting on the charge sheet , Baig 's lawyer A. Rehman said his client was falsely implicated . Baig was given the charge sheet on Monday in the first class judicial magistrate court . The case is now moved to the sessions . OPEN\n",
      "('Predicted:', 'O ORG_Others ORG_Others ORG_Others O O O O O O O O O LOC_Event LOC_Event LOC_Event O O O PER_Accused PER_Accused PER_Accused PER_Accused O O O O O O O O O O O O O O O O O O O O O O O O O O O O O ORG_Others O O O O O O O O O O O O O O O O PER_Accused PER_Accused O PER_Others PER_Others O O O O O O O PER_Accused O O O O O O O O O O O O O O O O O O LOC_Others O O O O O O LOC_Others O O O O LOC_Others O O O O O LOC_Others LOC_Others O LOC_Others O LOC_Others O O O O O O O O O O O O O O O O O O O O O O O O O O PER_Accused PER_Accused O PER_Accused PER_Accused O O O O O PER_Accused PER_Accused O PER_Accused PER_Accused O O O O O O O O O O O O O O O O O O O O LOC_Event LOC_Event O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O PER_Accused O PER_Accused O O O O O O O O O O O O O O O O O O O LOC_Others O O O O O O LOC_Others O O O O O O O O O O O O O O O O O O PER_Accused O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O LOC_Others LOC_Others LOC_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O ORG_Others ORG_Others ORG_Others ORG_Others ORG_Others O O ORG_Accused O O O O O O O O O O O O O O O O O O O ORG_Accused ORG_Accused O O O O O O O O O O O O O O O O O O O LOC_Others LOC_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O PER_Accused PER_Accused PER_Accused O O O O O O O O O O O O O O O O O O O PER_Others PER_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O PER_Others PER_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O')\n",
      "('Correct:  ', 'O ORG_Others ORG_Others ORG_Others O O O O LOC_Others LOC_Others O O O LOC_Others LOC_Event LOC_Event O O O PER_Accused PER_Accused PER_Accused PER_Accused O O O O O O O O O O O O O O O O O LOC_Others O O O O O O O O O O O O O PER_Accused O O O O O O O O O O O O O O PER_Accused PER_Accused O PER_Accused PER_Accused O O O O O PER_Accused O PER_Accused O PER_Accused O O O O O O O O O O O O O O O O LOC_Others O O O O O O LOC_Others O PER_Accused O O LOC_Others O O O O O LOC_Others O O LOC_Others O LOC_Others O O O O O PER_Accused O O PER_Accused O O O O O O O O O O O O O O O O O PER_Accused PER_Accused O PER_Accused PER_Accused O O O O O PER_Accused PER_Accused O PER_Accused PER_Accused O PER_Accused O LOC_Others O O O O O O O O O O O O O O O O ORG_Victim ORG_Victim O O O O O O O O O O O PER_Accused O O O O O O O O O O O O O O O O O O O O PER_Accused O PER_Accused O O LOC_Others O O O O O O O O O O O O O O PER_Accused O LOC_Others O O O O O O O O O O O O O O O O O O O O O O O PER_Accused O PER_Accused O O O O O O ORG_Others ORG_Others ORG_Others O LOC_Others O O O O O O O O O O O LOC_Others O O O O O O O O LOC_Others O LOC_Others O O O O O O O O O O O O O O O PER_Accused O O O ORG_Victim ORG_Victim O O O O O O O O O O O O O O O O O O O O O O O O O O PER_Accused O O O O O O ORG_Accused ORG_Accused ORG_Accused ORG_Accused ORG_Accused O O ORG_Accused O O O O O O PER_Accused O O O O O O O O O O O O ORG_Accused ORG_Accused O O O O O PER_Accused O O O O PER_Accused O O O O O O O O LOC_Others O O O O O O O O O O O O O O O O O O O O O O O O O O O PER_Accused O O O O O O O O PER_Accused PER_Accused PER_Accused O PER_Accused PER_Accused PER_Accused O O O O O O O O O O O O O O O O ORG_Others ORG_Others O PER_Accused PER_Accused O O O O PER_Accused O O O O O O O O O PER_Accused PER_Accused PER_Accused O O O O O O O O O O O O O O PER_Accused O O PER_Others PER_Others O O O O O O O PER_Accused O O O O O O O O O O O O O O O O O O O O O O O O O')\n"
     ]
    }
   ],
   "source": [
    "example_sent = test_data[0]\n",
    "print(' '.join(sent2words(example_sent)))\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2tags(example_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [sent2features(s) for s in test_data]\n",
    "y_test = [sent2tags(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 888 ms, sys: 4 ms, total: 892 ms\n",
      "Wall time: 895 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Accused       0.00      0.00      0.00        73\n",
      "      Event       0.00      0.00      0.00        51\n",
      "LOC_Accused       0.20      0.01      0.01       163\n",
      "  LOC_Assoc       0.00      0.00      0.00        51\n",
      "  LOC_Event       0.53      0.42      0.47      1151\n",
      " LOC_Others       0.58      0.54      0.56      1721\n",
      " LOC_Victim       0.00      0.00      0.00        20\n",
      "   Location       0.33      0.02      0.04        95\n",
      "ORG_Accused       0.77      0.76      0.76       572\n",
      "  ORG_Assoc       0.00      0.00      0.00        10\n",
      " ORG_Others       0.65      0.56      0.60      2091\n",
      " ORG_Victim       0.00      0.00      0.00        65\n",
      "PER_Accused       0.62      0.38      0.47       808\n",
      "  PER_Assoc       0.00      0.00      0.00        64\n",
      " PER_Others       0.72      0.70      0.71      1769\n",
      " PER_Victim       0.38      0.24      0.30       306\n",
      "     Victim       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.60      0.51      0.55      9011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "num_test_data = len(y_test)\n",
    "print len(y_pred)\n",
    "print num_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "actual_tag_count = dict()\n",
    "matched_tag_count = dict()\n",
    "matched_tag_count['PER'] = 0\n",
    "actual_tag_count['PER'] = 0\n",
    "matched_tag_count['ORG'] = 0\n",
    "actual_tag_count['ORG'] = 0\n",
    "matched_tag_count['LOC'] = 0\n",
    "actual_tag_count['LOC'] = 0\n",
    "\n",
    "for i in xrange(num_test_data):\n",
    "    y_test_i = y_test[i]\n",
    "    y_pred_i = y_pred[i]\n",
    "    test_tag_length = len(y_test[i])\n",
    "    predicted_tag_length = len(y_pred[i])\n",
    "    for j in xrange(test_tag_length):\n",
    "        if y_test_i[j][:3] in actual_tag_count.keys():\n",
    "            actual_tag_count[y_test_i[j][:3]] = actual_tag_count[y_test_i[j][:3]] + 1\n",
    "        \n",
    "        if y_test_i[j][:3] == y_pred_i[j][:3]:\n",
    "            if y_test_i[j][:3] in matched_tag_count.keys():\n",
    "                matched_tag_count[y_test_i[j][:3]] = matched_tag_count[y_test_i[j][:3]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Accuracy Class Wise =====================\n",
      "\n",
      "Class    Matched Total %\n",
      "--------------------------\n",
      "PER: 1766 2579 68%\n",
      "ORG: 1629 2663 61%\n",
      "LOC: 2094 3265 64%\n"
     ]
    }
   ],
   "source": [
    "print('======= Accuracy Class Wise =====================\\n')\n",
    "\n",
    "print('Class    Matched Total %')\n",
    "print('--------------------------')\n",
    "print ('PER: '+str(matched_tag_count['PER'])+ ' ' + str(actual_tag_count['PER'])+ ' ' + str(matched_tag_count['PER']*100/actual_tag_count['PER'])+'%')\n",
    "print ('ORG: '+str(matched_tag_count['ORG']) + ' '+ str(actual_tag_count['ORG'])+ ' '+str(matched_tag_count['ORG']*100/actual_tag_count['ORG'])+'%')\n",
    "print ('LOC: '+str(matched_tag_count['LOC']) + ' '+ str(actual_tag_count['LOC'])+ ' '+str(matched_tag_count['LOC']*100/actual_tag_count['LOC'])+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
